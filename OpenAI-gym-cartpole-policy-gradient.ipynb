{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Algorithm for CartPole OpenAI Gym\n",
    "This file implements the most popular Reinforcement Learning algorithm as a solution to the intro-to-RL CartPole problem from the OpenAI Gym (https://openai.com/requests-for-research/#cartpole), the \"*policy gradient algorithm*\".\n",
    "\n",
    "For a nice overview of Policy Gradients, which I used as the basis of this notebook, as always turn to Karpathy's [excellent article](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-08-09 11:54:19,006] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highs: [  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "Lows:  [ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(\"Highs:\", env.observation_space.high)\n",
    "print(\"Lows: \", env.observation_space.low)\n",
    "\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0033488 ,  0.04065428,  0.01961779,  0.02868828])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I'm just keeping track of my personal best. This has to be updated manually.\n",
    "# ... When I got this, it converged after ~75 batches, w/ these params:\n",
    "#   discount_factor = 0.9\n",
    "#   batch_size = 100\n",
    "#   learning_rate = 0.15\n",
    "#   max_episode_length = 5000\n",
    "# This seems to usually converge after between 70 and 200 batches.\n",
    "personal_best_reward = 5000\n",
    "personal_best_weight = np.array([  6.94065202,  83.09736598,  54.54100834,  68.92081203])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "discount_factor = 0.9  # Reward decay for rewards given after the action.\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "max_episode_length = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_range(a,b,shape):\n",
    "    return (b-a) * np.random.random(shape) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the initial weight vector, of the same shape as the input.\n",
    "W = random_range(-1,1,env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    from scipy.special import expit\n",
    "    return expit(x)  # sigmoid \"squashing\" function to interval [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.4918821789509345)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_forward_step(W,x):\n",
    "    ''' Simplest model ever: Just the linear multiplication, i.e. dot product! '''\n",
    "    y_prob = sigmoid(np.dot(W,x))\n",
    "    action = 1 if np.random.uniform() < y_prob else 0\n",
    "    return action, y_prob\n",
    "model_forward_step(W,observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00301392,  0.03658885,  0.01765602,  0.02581946])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_backward_step(x,y_prob,action_taken,reward):\n",
    "    ''' Calculate dreward_dW:\n",
    "    If reward is positive, we want to make the *action we took* *more likely*, if negative, make it less likely.\n",
    "    So if reward is positive, we want to increase y_prob to be more towards action_taken, by reward amount.\n",
    "    So our gradient will be how to adjust W to make y_prob more like action_taken. *reward.\n",
    "    '''\n",
    "    # Assume action_taken = 1, y_prob = 0.9, reward = +1\n",
    "    chance = action_taken-y_prob  # 0.1\n",
    "    dreward_dyprob = chance*reward # \n",
    "    \n",
    "    dyprob_dW = x\n",
    "    dreward_dW = dreward_dyprob*dyprob_dW\n",
    "    return dreward_dW\n",
    "model_backward_step(observation, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounted Rewards:\n",
    "$$ R_{t} = \\sum_{k=0}^{âˆž}\\gamma^k r_{t+k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.22,  2.2 ,  2.  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    current_gamma = discount_factor\n",
    "    reverse_discounted_sum = 0\n",
    "    for t in reversed(xrange(0,len(rewards))):\n",
    "        reverse_discounted_sum *= discount_factor\n",
    "        reverse_discounted_sum += float(rewards[t])\n",
    "        discounted_rewards[t] = reverse_discounted_sum\n",
    "    return discounted_rewards\n",
    "discount_rewards([2,2.,2], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the initial weight vector, of the same shape as the input.\n",
    "W = random_range(-1,1,env.observation_space.shape)\n",
    "total_reward = 0.0\n",
    "episode_number = 0\n",
    "batch_number = 0\n",
    "Ws = [np.copy(W)]  # Just to keep track so you can try playing with the weights from each step.\n",
    "batch_rewards = []  # To keep track for printing, plotting, etc.\n",
    "running_avg_rewards = [] # To keep track for printing, plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Avg Batch reward: 26.97, running avg: 26.97\n",
      "1: Avg Batch reward: 92.25, running avg: 59.61\n",
      "2: Avg Batch reward: 166.14, running avg: 95.12\n",
      "3: Avg Batch reward: 92.67, running avg: 94.5075\n",
      "4: Avg Batch reward: 80.48, running avg: 91.702\n",
      "5: Avg Batch reward: 100.86, running avg: 93.2283\n",
      "6: Avg Batch reward: 127.92, running avg: 98.1843\n",
      "7: Avg Batch reward: 921.5, running avg: 201.099\n",
      "8: Avg Batch reward: 618.21, running avg: 247.444\n",
      "9: Avg Batch reward: 72.44, running avg: 229.944\n",
      "10: Avg Batch reward: 81.66, running avg: 216.464\n",
      "11: Avg Batch reward: 79.62, running avg: 205.06\n",
      "12: Avg Batch reward: 87.05, running avg: 195.982\n",
      "13: Avg Batch reward: 82.4, running avg: 187.869\n",
      "14: Avg Batch reward: 86.47, running avg: 181.109\n",
      "15: Avg Batch reward: 93.04, running avg: 175.605\n",
      "16: Avg Batch reward: 91.29, running avg: 170.645\n",
      "17: Avg Batch reward: 91.01, running avg: 166.221\n",
      "18: Avg Batch reward: 100.25, running avg: 162.749\n",
      "19: Avg Batch reward: 100.62, running avg: 159.643\n",
      "20: Avg Batch reward: 102.15, running avg: 156.905\n",
      "21: Avg Batch reward: 91.52, running avg: 153.933\n",
      "22: Avg Batch reward: 87.13, running avg: 151.028\n",
      "23: Avg Batch reward: 103.41, running avg: 149.044\n",
      "24: Avg Batch reward: 99.57, running avg: 147.065\n",
      "25: Avg Batch reward: 96.48, running avg: 145.12\n",
      "26: Avg Batch reward: 109.49, running avg: 143.8\n",
      "27: Avg Batch reward: 126.63, running avg: 143.187\n",
      "28: Avg Batch reward: 138.82, running avg: 143.036\n",
      "29: Avg Batch reward: 151.97, running avg: 143.334\n",
      "30: Avg Batch reward: 191.59, running avg: 144.891\n",
      "31: Avg Batch reward: 242.04, running avg: 147.927\n",
      "32: Avg Batch reward: 846.32, running avg: 169.09\n",
      "33: Avg Batch reward: 1181.0, running avg: 198.851\n",
      "34: Avg Batch reward: 638.7, running avg: 211.418\n",
      "Stopping Looping!\n",
      "Interrupted loop (35): interrupted episode reward: 4.6092e+05, info: {}\n",
      "Num Batches: 36, Avg Reward: 20554.5\n",
      "Final Weights: [  0.13401846   9.91894307  31.67810733  47.79741546]\n"
     ]
    }
   ],
   "source": [
    "# Start by printing any previous runs so you can start & stop w/out losing\n",
    "# output history.\n",
    "for i in range(len(batch_rewards)):\n",
    "    print(\"{0}: Avg Batch reward: {1:.5}, running avg: {2:.6}\".format(i, batch_rewards[i], running_avg_rewards[i]))\n",
    "try:\n",
    "    while True:\n",
    "        gradient = np.zeros_like(W)\n",
    "        total_batch_reward = 0.0\n",
    "        for ep in range(0,batch_size):\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            total_episode_reward = 0\n",
    "            observations = []\n",
    "            rewards = []\n",
    "            y_probs = []\n",
    "            actions_taken = []\n",
    "            #for _ in range(max_episode_length):\n",
    "            while True:\n",
    "                #env.render()\n",
    "                action, y = model_forward_step(W,observation)\n",
    "                observations.append(observation)\n",
    "                y_probs.append(y)\n",
    "                actions_taken.append(action)\n",
    "\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                rewards.append(reward)\n",
    "                total_episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # End of the Episode\n",
    "            episode_number += 1\n",
    "            discounted_ep_rewards = discount_rewards(rewards, discount_factor)\n",
    "            # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_ep_rewards -= np.mean(discounted_ep_rewards)\n",
    "            discounted_ep_rewards /= np.std(discounted_ep_rewards)\n",
    "\n",
    "            ep_grad = np.zeros_like(W)\n",
    "            for i in range(0, len(observations)):\n",
    "                ep_grad += model_backward_step(observations[i],\n",
    "                                               y_probs[i],\n",
    "                                               actions_taken[i],\n",
    "                                               discounted_ep_rewards[i])\n",
    "\n",
    "            gradient += ep_grad\n",
    "            total_batch_reward += total_episode_reward\n",
    "\n",
    "        # End of batch\n",
    "        total_reward += total_batch_reward\n",
    "        running_avg_reward = total_reward/((batch_number+1)*batch_size)\n",
    "        batch_rewards.append(total_batch_reward/batch_size)\n",
    "        running_avg_rewards.append(running_avg_reward)\n",
    "\n",
    "        if (batch_number % 1) == 0:\n",
    "            print(\"{0}: Avg Batch reward: {1:.5}, running avg: {2:.6}\".format(batch_number, batch_rewards[batch_number], running_avg_rewards[batch_number]))\n",
    " \n",
    "        W += learning_rate * gradient\n",
    "        gradient = np.zeros_like(W) # reset batch gradient buffer\n",
    " \n",
    "        batch_number += 1\n",
    "        Ws.append(np.copy(W))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping Looping!\")\n",
    "    print(\"Interrupted loop ({0}): interrupted episode reward: {1:.5}, info: {2}\".format(batch_number, total_episode_reward, info))\n",
    "\n",
    "num_batches = batch_number+1\n",
    "average_reward = total_reward/num_batches\n",
    "print(\"Num Batches: {0}, Avg Reward: {1}\".format(num_batches,average_reward))\n",
    "print(\"Final Weights:\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.13401846,   9.91894307,  31.67810733,  47.79741546])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_model(W, num_steps=max_episode_length*2, num_test_episodes = 5):\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i_episode in range(num_test_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        print(\"{0}/{1}:\".format(i_episode, num_test_episodes))\n",
    "        for _ in range(num_steps):\n",
    "            env.render()  # I don't think you can get this to render from MyBinder. :(\n",
    "            action,_ = model_forward_step(W,observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(\"{0}/{1}: Episode Reward: {2}\".format(i_episode, num_test_episodes, episode_reward))\n",
    "        total_reward += episode_reward\n",
    "    average_reward = total_reward/num_test_episodes\n",
    "    return total_reward,average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1:\n",
      "0/1: Episode Reward: 10000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000.0, 10000.0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(Ws[-1], num_steps=10000, num_test_episodes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3:\n",
      "0/3: Episode Reward: 1322.0\n",
      "1/3:\n",
      "1/3: Episode Reward: 946.0\n",
      "2/3:\n",
      "2/3: Episode Reward: 1646.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3914.0, 1304.6666666666667)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(Ws[33], num_steps=10000, num_test_episodes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2:\n",
      "0/2: Episode Reward: 10000.0\n",
      "1/2:\n",
      "1/2: Episode Reward: 10000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000.0, 10000.0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(personal_best_weight, num_steps=10000, num_test_episodes = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Observations\n",
    "- Often the model will run for a long time at a very low initial reward and not improve dozens of batches. I assume this is because it happens to start off in a bad part of the weight space and struggles to find any improvements.\n",
    "- Also, almost always while training the model will make very saw-toothed improvements, getting better and better until it very drastically drops back down to ~200 or 300. Sometimes it even hits the max runs per episode, but then the very next update brings it back down. I don't know why this happens, but it still seems to *eventually* converge (as far as I can tell).\n",
    "- For example of the above, in one run I saw this:\n",
    "```\n",
    "135: Avg Batch reward: 349.95, running avg: 135.458\n",
    "136: Avg Batch reward: 334.93, running avg: 136.914\n",
    "137: Avg Batch reward: 486.63, running avg: 139.448\n",
    "138: Avg Batch reward: 702.71, running avg: 143.5\n",
    "139: Avg Batch reward: 744.29, running avg: 147.791\n",
    "140: Avg Batch reward: 5000.0, running avg: 182.204\n",
    "141: Avg Batch reward: 4162.8, running avg: 210.236\n",
    "142: Avg Batch reward: 762.46, running avg: 214.098\n",
    "143: Avg Batch reward: 384.87, running avg: 215.284\n",
    "144: Avg Batch reward: 645.32, running avg: 218.25\n",
    "```\n",
    "- Sometimes I'll see it come and go from `5000.0` like above. Does this mean my learning rate ($\\alpha$) is too large?\n",
    "```\n",
    "31: Avg Batch reward: 487.95, running avg: 341.933\n",
    "32: Avg Batch reward: 265.17, running avg: 339.607\n",
    "33: Avg Batch reward: 5000.0, running avg: 476.677\n",
    "34: Avg Batch reward: 5000.0, running avg: 605.915\n",
    "35: Avg Batch reward: 3584.6, running avg: 688.657\n",
    "36: Avg Batch reward: 153.38, running avg: 674.19\n",
    "37: Avg Batch reward: 169.31, running avg: 660.903\n",
    "```\n",
    "\n",
    "\n",
    "--> Ah, actually, come to think of it, it's a bug to apply any gradient at all when we've hit the max number of steps in each episode in a batch. Doing so means you're randomly rewarding half and randomly punishing half, which will take you away from the peak you're on and emphasize unimportant variations.\n",
    "- I dunno if the solution is to just not set a max_steps or to lessen the learning rate the closer you are to the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
