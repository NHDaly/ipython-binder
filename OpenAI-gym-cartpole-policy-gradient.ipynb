{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Algorithm for CartPole OpenAI Gym\n",
    "This file implements the most popular Reinforcement Learning algorithm as a solution to the intro-to-RL CartPole problem from the OpenAI Gym (https://openai.com/requests-for-research/#cartpole), the \"*policy gradient algorithm*\".\n",
    "\n",
    "For a nice overview of Policy Gradients, which I used as the basis of this notebook, as always turn to Karpathy's [excellent article](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-08-18 22:11:34,427] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highs: [  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "Lows:  [ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(\"Highs:\", env.observation_space.high)\n",
    "print(\"Lows: \", env.observation_space.low)\n",
    "\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00372483,  0.01448213,  0.03009171, -0.03213908])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I'm just keeping track of my personal best. This has to be updated manually.\n",
    "# ... When I got this, it converged after ~75 batches, w/ these params:\n",
    "#   discount_factor = 0.9\n",
    "#   batch_size = 100\n",
    "#   learning_rate = 0.15\n",
    "#   max_episode_length = 5000\n",
    "# This seems to usually converge after between 70 and 200 batches.\n",
    "personal_best_reward = 5000\n",
    "personal_best_weight = np.array([  6.94065202,  83.09736598,  54.54100834,  68.92081203])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "discount_factor = 0.9  # Reward decay for rewards given after the action.\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "max_episode_length = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_range(a,b,shape):\n",
    "    return (b-a) * np.random.random(shape) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the initial weight vector, of the same shape as the input.\n",
    "W = random_range(-1,1,env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    from scipy.special import expit\n",
    "    return expit(x)  # sigmoid \"squashing\" function to interval [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.50613593729437134)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_forward_step(W,x):\n",
    "    ''' Simplest model ever: Just the linear multiplication, i.e. dot product! '''\n",
    "    y_prob = sigmoid(np.dot(W,x))\n",
    "    action = 1 if np.random.uniform() < y_prob else 0\n",
    "    return action, y_prob\n",
    "model_forward_step(W,observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00335235,  0.01303392,  0.02708254, -0.02892517])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_backward_step(x,y_prob,action_taken,reward):\n",
    "    ''' Calculate dreward_dW:\n",
    "    If reward is positive, we want to make the *action we took* *more likely*, if negative, make it less likely.\n",
    "    So if reward is positive, we want to increase y_prob to be more towards action_taken, by reward amount.\n",
    "    So our gradient will be how to adjust W to make y_prob more like action_taken. *reward.\n",
    "    '''\n",
    "    # Assume action_taken = 1, y_prob = 0.9, reward = +1\n",
    "    chance = action_taken-y_prob  # 0.1\n",
    "    dreward_dyprob = chance*reward # \n",
    "    \n",
    "    dyprob_dW = x\n",
    "    dreward_dW = dreward_dyprob*dyprob_dW\n",
    "    return dreward_dW\n",
    "model_backward_step(observation, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounted Rewards:\n",
    "$$ R_{t} = \\sum_{k=0}^{âˆž}\\gamma^k r_{t+k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.22,  2.2 ,  2.  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    current_gamma = discount_factor\n",
    "    reverse_discounted_sum = 0\n",
    "    for t in reversed(xrange(0,len(rewards))):\n",
    "        reverse_discounted_sum *= discount_factor\n",
    "        reverse_discounted_sum += float(rewards[t])\n",
    "        discounted_rewards[t] = reverse_discounted_sum\n",
    "    return discounted_rewards\n",
    "discount_rewards([2,2.,2], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the initial weight vector, of the same shape as the input.\n",
    "W = random_range(-1,1,env.observation_space.shape)\n",
    "total_reward = 0.0\n",
    "episode_number = 0\n",
    "batch_number = 0\n",
    "Ws = [np.copy(W)]  # Just to keep track so you can try playing with the weights from each step.\n",
    "batch_rewards = []  # To keep track for printing, plotting, etc.\n",
    "running_avg_rewards = [] # To keep track for printing, plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Avg Batch reward: 14.0, running avg: 14.0\n",
      "1: Avg Batch reward: 64.74, running avg: 39.37\n",
      "2: Avg Batch reward: 83.77, running avg: 54.17\n",
      "3: Avg Batch reward: 123.42, running avg: 71.4825\n",
      "4: Avg Batch reward: 100.39, running avg: 77.264\n",
      "5: Avg Batch reward: 110.91, running avg: 82.8717\n",
      "6: Avg Batch reward: 110.11, running avg: 86.7629\n",
      "7: Avg Batch reward: 172.58, running avg: 97.49\n",
      "8: Avg Batch reward: 158.52, running avg: 104.271\n",
      "9: Avg Batch reward: 201.12, running avg: 113.956\n",
      "Stopping Looping!\n",
      "Interrupted loop (10): interrupted episode reward: 218.0, info: {}\n",
      "Num Batches: 11, Avg Reward: 10359.6363636\n",
      "Final Weights: [  7.13811333   2.32815943  12.79017101  34.13093733]\n"
     ]
    }
   ],
   "source": [
    "# Start by printing any previous runs so you can start & stop w/out losing\n",
    "# output history.\n",
    "for i in range(len(batch_rewards)):\n",
    "    print(\"{0}: Avg Batch reward: {1:.5}, running avg: {2:.6}\".format(i, batch_rewards[i], running_avg_rewards[i]))\n",
    "try:\n",
    "    while True:\n",
    "        gradient = np.zeros_like(W)\n",
    "        total_batch_reward = 0.0\n",
    "        for ep in range(0,batch_size):\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            total_episode_reward = 0\n",
    "            observations = []\n",
    "            rewards = []\n",
    "            y_probs = []\n",
    "            actions_taken = []\n",
    "            #for _ in range(max_episode_length):\n",
    "            while True:\n",
    "                #env.render()\n",
    "                action, y = model_forward_step(W,observation)\n",
    "                observations.append(observation)\n",
    "                y_probs.append(y)\n",
    "                actions_taken.append(action)\n",
    "\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                rewards.append(reward)\n",
    "                total_episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # End of the Episode\n",
    "            episode_number += 1\n",
    "            discounted_ep_rewards = discount_rewards(rewards, discount_factor)\n",
    "            # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_ep_rewards -= np.mean(discounted_ep_rewards)\n",
    "            discounted_ep_rewards /= np.std(discounted_ep_rewards)\n",
    "\n",
    "            ep_grad = np.zeros_like(W)\n",
    "            for i in range(0, len(observations)):\n",
    "                ep_grad += model_backward_step(observations[i],\n",
    "                                               y_probs[i],\n",
    "                                               actions_taken[i],\n",
    "                                               discounted_ep_rewards[i])\n",
    "\n",
    "            gradient += ep_grad\n",
    "            total_batch_reward += total_episode_reward\n",
    "\n",
    "        # End of batch\n",
    "        total_reward += total_batch_reward\n",
    "        running_avg_reward = total_reward/((batch_number+1)*batch_size)\n",
    "        batch_rewards.append(total_batch_reward/batch_size)\n",
    "        running_avg_rewards.append(running_avg_reward)\n",
    "\n",
    "        if (batch_number % 1) == 0:\n",
    "            print(\"{0}: Avg Batch reward: {1:.5}, running avg: {2:.6}\".format(batch_number, batch_rewards[batch_number], running_avg_rewards[batch_number]))\n",
    " \n",
    "        W += learning_rate * gradient\n",
    "        gradient = np.zeros_like(W) # reset batch gradient buffer\n",
    " \n",
    "        batch_number += 1\n",
    "        Ws.append(np.copy(W))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping Looping!\")\n",
    "    print(\"Interrupted loop ({0}): interrupted episode reward: {1:.5}, info: {2}\".format(batch_number, total_episode_reward, info))\n",
    "\n",
    "num_batches = batch_number+1\n",
    "average_reward = total_reward/num_batches\n",
    "print(\"Num Batches: {0}, Avg Reward: {1}\".format(num_batches,average_reward))\n",
    "print(\"Final Weights:\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.13811333,   2.32815943,  12.79017101,  34.13093733])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ws[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_model(W, num_steps=max_episode_length*2, num_test_episodes = 5):\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i_episode in range(num_test_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        print(\"{0}/{1}:\".format(i_episode, num_test_episodes))\n",
    "        for _ in range(num_steps):\n",
    "            env.render()  # I don't think you can get this to render from MyBinder. :(\n",
    "            action,_ = model_forward_step(W,observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(\"{0}/{1}: Episode Reward: {2}\".format(i_episode, num_test_episodes, episode_reward))\n",
    "        total_reward += episode_reward\n",
    "    average_reward = total_reward/num_test_episodes\n",
    "    return total_reward,average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100:\n",
      "0/100: Episode Reward: 18.0\n",
      "1/100:\n",
      "1/100: Episode Reward: 25.0\n",
      "2/100:\n",
      "2/100: Episode Reward: 34.0\n",
      "3/100:\n",
      "3/100: Episode Reward: 27.0\n",
      "4/100:\n",
      "4/100: Episode Reward: 12.0\n",
      "5/100:\n",
      "5/100: Episode Reward: 63.0\n",
      "6/100:\n",
      "6/100: Episode Reward: 60.0\n",
      "7/100:\n",
      "7/100: Episode Reward: 15.0\n",
      "8/100:\n",
      "8/100: Episode Reward: 17.0\n",
      "9/100:\n",
      "9/100: Episode Reward: 23.0\n",
      "10/100:\n",
      "10/100: Episode Reward: 54.0\n",
      "11/100:\n",
      "11/100: Episode Reward: 11.0\n",
      "12/100:\n",
      "12/100: Episode Reward: 18.0\n",
      "13/100:\n",
      "13/100: Episode Reward: 83.0\n",
      "14/100:\n",
      "14/100: Episode Reward: 21.0\n",
      "15/100:\n",
      "15/100: Episode Reward: 29.0\n",
      "16/100:\n",
      "16/100: Episode Reward: 100.0\n",
      "17/100:\n",
      "17/100: Episode Reward: 15.0\n",
      "18/100:\n",
      "18/100: Episode Reward: 48.0\n",
      "19/100:\n",
      "19/100: Episode Reward: 49.0\n",
      "20/100:\n",
      "20/100: Episode Reward: 14.0\n",
      "21/100:\n",
      "21/100: Episode Reward: 33.0\n",
      "22/100:\n",
      "22/100: Episode Reward: 13.0\n",
      "23/100:\n",
      "23/100: Episode Reward: 23.0\n",
      "24/100:\n",
      "24/100: Episode Reward: 14.0\n",
      "25/100:\n",
      "25/100: Episode Reward: 18.0\n",
      "26/100:\n",
      "26/100: Episode Reward: 22.0\n",
      "27/100:\n",
      "27/100: Episode Reward: 15.0\n",
      "28/100:\n",
      "28/100: Episode Reward: 60.0\n",
      "29/100:\n",
      "29/100: Episode Reward: 17.0\n",
      "30/100:\n",
      "30/100: Episode Reward: 41.0\n",
      "31/100:\n",
      "31/100: Episode Reward: 35.0\n",
      "32/100:\n",
      "32/100: Episode Reward: 50.0\n",
      "33/100:\n",
      "33/100: Episode Reward: 34.0\n",
      "34/100:\n",
      "34/100: Episode Reward: 66.0\n",
      "35/100:\n",
      "35/100: Episode Reward: 79.0\n",
      "36/100:\n",
      "36/100: Episode Reward: 19.0\n",
      "37/100:\n",
      "37/100: Episode Reward: 46.0\n",
      "38/100:\n",
      "38/100: Episode Reward: 39.0\n",
      "39/100:\n",
      "39/100: Episode Reward: 29.0\n",
      "40/100:\n",
      "40/100: Episode Reward: 35.0\n",
      "41/100:\n",
      "41/100: Episode Reward: 13.0\n",
      "42/100:\n",
      "42/100: Episode Reward: 101.0\n",
      "43/100:\n",
      "43/100: Episode Reward: 34.0\n",
      "44/100:\n",
      "44/100: Episode Reward: 19.0\n",
      "45/100:\n",
      "45/100: Episode Reward: 52.0\n",
      "46/100:\n",
      "46/100: Episode Reward: 13.0\n",
      "47/100:\n",
      "47/100: Episode Reward: 14.0\n",
      "48/100:\n",
      "48/100: Episode Reward: 39.0\n",
      "49/100:\n",
      "49/100: Episode Reward: 27.0\n",
      "50/100:\n",
      "50/100: Episode Reward: 20.0\n",
      "51/100:\n",
      "51/100: Episode Reward: 35.0\n",
      "52/100:\n",
      "52/100: Episode Reward: 23.0\n",
      "53/100:\n",
      "53/100: Episode Reward: 29.0\n",
      "54/100:\n",
      "54/100: Episode Reward: 20.0\n",
      "55/100:\n",
      "55/100: Episode Reward: 22.0\n",
      "56/100:\n",
      "56/100: Episode Reward: 11.0\n",
      "57/100:\n",
      "57/100: Episode Reward: 48.0\n",
      "58/100:\n",
      "58/100: Episode Reward: 29.0\n",
      "59/100:\n",
      "59/100: Episode Reward: 25.0\n",
      "60/100:\n",
      "60/100: Episode Reward: 25.0\n",
      "61/100:\n",
      "61/100: Episode Reward: 19.0\n",
      "62/100:\n",
      "62/100: Episode Reward: 62.0\n",
      "63/100:\n",
      "63/100: Episode Reward: 17.0\n",
      "64/100:\n",
      "64/100: Episode Reward: 71.0\n",
      "65/100:\n",
      "65/100: Episode Reward: 27.0\n",
      "66/100:\n",
      "66/100: Episode Reward: 39.0\n",
      "67/100:\n",
      "67/100: Episode Reward: 75.0\n",
      "68/100:\n",
      "68/100: Episode Reward: 16.0\n",
      "69/100:\n",
      "69/100: Episode Reward: 17.0\n",
      "70/100:\n",
      "70/100: Episode Reward: 37.0\n",
      "71/100:\n",
      "71/100: Episode Reward: 25.0\n",
      "72/100:\n",
      "72/100: Episode Reward: 18.0\n",
      "73/100:\n",
      "73/100: Episode Reward: 29.0\n",
      "74/100:\n",
      "74/100: Episode Reward: 12.0\n",
      "75/100:\n",
      "75/100: Episode Reward: 33.0\n",
      "76/100:\n",
      "76/100: Episode Reward: 91.0\n",
      "77/100:\n",
      "77/100: Episode Reward: 50.0\n",
      "78/100:\n",
      "78/100: Episode Reward: 17.0\n",
      "79/100:\n",
      "79/100: Episode Reward: 27.0\n",
      "80/100:\n",
      "80/100: Episode Reward: 15.0\n",
      "81/100:\n",
      "81/100: Episode Reward: 38.0\n",
      "82/100:\n",
      "82/100: Episode Reward: 17.0\n",
      "83/100:\n",
      "83/100: Episode Reward: 41.0\n",
      "84/100:\n",
      "84/100: Episode Reward: 18.0\n",
      "85/100:\n",
      "85/100: Episode Reward: 67.0\n",
      "86/100:\n",
      "86/100: Episode Reward: 25.0\n",
      "87/100:\n",
      "87/100: Episode Reward: 18.0\n",
      "88/100:\n",
      "88/100: Episode Reward: 13.0\n",
      "89/100:\n",
      "89/100: Episode Reward: 29.0\n",
      "90/100:\n",
      "90/100: Episode Reward: 50.0\n",
      "91/100:\n",
      "91/100: Episode Reward: 43.0\n",
      "92/100:\n",
      "92/100: Episode Reward: 65.0\n",
      "93/100:\n",
      "93/100: Episode Reward: 52.0\n",
      "94/100:\n",
      "94/100: Episode Reward: 26.0\n",
      "95/100:\n",
      "95/100: Episode Reward: 22.0\n",
      "96/100:\n",
      "96/100: Episode Reward: 19.0\n",
      "97/100:\n",
      "97/100: Episode Reward: 27.0\n",
      "98/100:\n",
      "98/100: Episode Reward: 70.0\n",
      "99/100:\n",
      "99/100: Episode Reward: 35.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3395.0, 33.95)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(random_range(-1,1,env.observation_space.shape), num_steps=10000, num_test_episodes = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8790c41ec9ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrender_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_test_episodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "render_model(Ws[33], num_steps=10000, num_test_episodes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2:\n",
      "0/2: Episode Reward: 10000.0\n",
      "1/2:\n",
      "1/2: Episode Reward: 10000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000.0, 10000.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(personal_best_weight, num_steps=10000, num_test_episodes = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Observations\n",
    "- Often the model will run for a long time at a very low initial reward and not improve dozens of batches. I assume this is because it happens to start off in a bad part of the weight space and struggles to find any improvements.\n",
    "- Also, almost always while training the model will make very saw-toothed improvements, getting better and better until it very drastically drops back down to ~200 or 300. Sometimes it even hits the max runs per episode, but then the very next update brings it back down. I don't know why this happens, but it still seems to *eventually* converge (as far as I can tell).\n",
    "- For example of the above, in one run I saw this:\n",
    "```\n",
    "135: Avg Batch reward: 349.95, running avg: 135.458\n",
    "136: Avg Batch reward: 334.93, running avg: 136.914\n",
    "137: Avg Batch reward: 486.63, running avg: 139.448\n",
    "138: Avg Batch reward: 702.71, running avg: 143.5\n",
    "139: Avg Batch reward: 744.29, running avg: 147.791\n",
    "140: Avg Batch reward: 5000.0, running avg: 182.204\n",
    "141: Avg Batch reward: 4162.8, running avg: 210.236\n",
    "142: Avg Batch reward: 762.46, running avg: 214.098\n",
    "143: Avg Batch reward: 384.87, running avg: 215.284\n",
    "144: Avg Batch reward: 645.32, running avg: 218.25\n",
    "```\n",
    "- Sometimes I'll see it come and go from `5000.0` like above. Does this mean my learning rate ($\\alpha$) is too large?\n",
    "```\n",
    "31: Avg Batch reward: 487.95, running avg: 341.933\n",
    "32: Avg Batch reward: 265.17, running avg: 339.607\n",
    "33: Avg Batch reward: 5000.0, running avg: 476.677\n",
    "34: Avg Batch reward: 5000.0, running avg: 605.915\n",
    "35: Avg Batch reward: 3584.6, running avg: 688.657\n",
    "36: Avg Batch reward: 153.38, running avg: 674.19\n",
    "37: Avg Batch reward: 169.31, running avg: 660.903\n",
    "```\n",
    "\n",
    "\n",
    "--> Ah, actually, come to think of it, it's a bug to apply any gradient at all when we've hit the max number of steps in each episode in a batch. Doing so means you're randomly rewarding half and randomly punishing half, which will take you away from the peak you're on and emphasize unimportant variations.\n",
    "- I dunno if the solution is to just not set a max_steps or to lessen the learning rate the closer you are to the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
