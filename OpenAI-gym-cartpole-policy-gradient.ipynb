{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Algorithm for CartPole OpenAI Gym\n",
    "This file implements the most popular Reinforcement Learning algorithm as a solution to the intro-to-RL CartPole problem from the OpenAI Gym (https://openai.com/requests-for-research/#cartpole), the \"*policy gradient algorithm*\".\n",
    "\n",
    "For a nice overview of Policy Gradients, which I used as the basis of this notebook, as always turn to Karpathy's [excellent article](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-08-08 23:41:54,893] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highs: [  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "Lows:  [ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(\"Highs:\", env.observation_space.high)\n",
    "print(\"Lows: \", env.observation_space.low)\n",
    "\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01279498,  0.00102932,  0.04406286, -0.02619262])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I'm just keeping track of my personal best. This has to be updated manually.\n",
    "# ... When I got this, it converged after ~75 batches, w/ these params:\n",
    "#   discount_factor = 0.9\n",
    "#   batch_size = 100\n",
    "#   learning_rate = 0.15\n",
    "#   max_episode_length = 5000\n",
    "# This seems to usually converge after between 70 and 200 batches.\n",
    "personal_best_reward = 5000\n",
    "personal_best_weight = np.array([  6.94065202,  83.09736598,  54.54100834,  68.92081203])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "discount_factor = 0.9\n",
    "batch_size = 100\n",
    "learning_rate = 0.15\n",
    "max_episode_length = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_range(a,b,shape):\n",
    "    return (b-a) * np.random.random(shape) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the initial weight vector, of the same shape as the input.\n",
    "W = random_range(-1,1,env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    from scipy.special import expit\n",
    "    return expit(x)  # sigmoid \"squashing\" function to interval [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.48689040930959387)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_forward_step(W,x):\n",
    "    ''' Simplest model ever: Just the linear multiplication, i.e. dot product! '''\n",
    "    y_prob = sigmoid(np.dot(W,x))\n",
    "    action = 1 if np.random.uniform() < y_prob else 0\n",
    "    return action, y_prob\n",
    "model_forward_step(W,observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01151548,  0.00092639,  0.03965657, -0.02357335])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_backward_step(x,y_prob,action_taken,reward):\n",
    "    ''' Calculate dreward_dW:\n",
    "    If reward is positive, we want to make the *action we took* *more likely*, if negative, make it less likely.\n",
    "    So if reward is positive, we want to increase y_prob to be more towards action_taken, by reward amount.\n",
    "    So our gradient will be how to adjust W to make y_prob more like action_taken. *reward.\n",
    "    '''\n",
    "    # Assume action_taken = 1, y_prob = 0.9, reward = +1\n",
    "    chance = action_taken-y_prob  # 0.1\n",
    "    dreward_dyprob = chance*reward # \n",
    "    \n",
    "    dyprob_dW = x\n",
    "    dreward_dW = dreward_dyprob*dyprob_dW\n",
    "    return dreward_dW\n",
    "model_backward_step(observation, 0.1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounted Rewards:\n",
    "$$ R_{t} = \\sum_{k=0}^{âˆž}\\gamma^k r_{t+k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.22,  2.2 ,  2.  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    current_gamma = discount_factor\n",
    "    reverse_discounted_sum = 0\n",
    "    for t in reversed(xrange(0,len(rewards))):\n",
    "        reverse_discounted_sum *= discount_factor\n",
    "        reverse_discounted_sum += float(rewards[t])\n",
    "        discounted_rewards[t] = reverse_discounted_sum\n",
    "    return discounted_rewards\n",
    "discount_rewards([2,2.,2], 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the initial weight vector, of the same shape as the input.\n",
    "W = random_range(-1,1,env.observation_space.shape)\n",
    "total_reward = 0.0\n",
    "episode_number = 0\n",
    "batch_number = 0\n",
    "Ws = []  # Just to keep track so you can try playing with the weights from each step.\n",
    "batch_rewards = []  # To keep track for printing, plotting, etc.\n",
    "running_avg_rewards = [] # To keep track for printing, plotting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Avg Batch reward: 21.8, running avg: 21.8\n",
      "1: Avg Batch reward: 79.85, running avg: 50.825\n",
      "2: Avg Batch reward: 220.31, running avg: 107.32\n",
      "3: Avg Batch reward: 66.39, running avg: 97.0875\n",
      "4: Avg Batch reward: 71.0, running avg: 91.87\n",
      "5: Avg Batch reward: 71.58, running avg: 88.4883\n",
      "6: Avg Batch reward: 77.09, running avg: 86.86\n",
      "7: Avg Batch reward: 75.19, running avg: 85.4013\n",
      "8: Avg Batch reward: 77.23, running avg: 84.4933\n",
      "9: Avg Batch reward: 81.9, running avg: 84.234\n",
      "10: Avg Batch reward: 81.27, running avg: 83.9645\n",
      "11: Avg Batch reward: 87.69, running avg: 91.9364\n",
      "12: Avg Batch reward: 92.77, running avg: 92.0058\n",
      "13: Avg Batch reward: 94.96, running avg: 92.2331\n",
      "14: Avg Batch reward: 94.42, running avg: 92.3893\n",
      "15: Avg Batch reward: 105.63, running avg: 93.272\n",
      "16: Avg Batch reward: 128.75, running avg: 95.4894\n",
      "17: Avg Batch reward: 161.38, running avg: 99.3653\n",
      "18: Avg Batch reward: 311.85, running avg: 111.17\n",
      "19: Avg Batch reward: 223.82, running avg: 117.099\n",
      "20: Avg Batch reward: 141.82, running avg: 118.335\n",
      "21: Avg Batch reward: 170.73, running avg: 120.83\n",
      "22: Avg Batch reward: 805.29, running avg: 151.942\n",
      "23: Avg Batch reward: 207.32, running avg: 154.35\n",
      "24: Avg Batch reward: 239.85, running avg: 157.912\n",
      "25: Avg Batch reward: 343.75, running avg: 165.346\n",
      "26: Avg Batch reward: 281.35, running avg: 169.807\n",
      "27: Avg Batch reward: 270.73, running avg: 173.545\n",
      "28: Avg Batch reward: 362.58, running avg: 180.296\n",
      "29: Avg Batch reward: 5000.0, running avg: 346.493\n",
      "30: Avg Batch reward: 1414.6, running avg: 382.097\n",
      "31: Avg Batch reward: 316.77, running avg: 379.99\n",
      "32: Avg Batch reward: 185.19, running avg: 373.902\n",
      "33: Avg Batch reward: 204.22, running avg: 368.761\n",
      "34: Avg Batch reward: 246.89, running avg: 365.176\n",
      "35: Avg Batch reward: 316.71, running avg: 363.791\n",
      "36: Avg Batch reward: 199.71, running avg: 359.234\n",
      "37: Avg Batch reward: 229.42, running avg: 355.725\n",
      "38: Avg Batch reward: 214.28, running avg: 352.003\n",
      "39: Avg Batch reward: 334.05, running avg: 351.543\n",
      "40: Avg Batch reward: 205.47, running avg: 347.891\n",
      "41: Avg Batch reward: 175.04, running avg: 343.675\n",
      "42: Avg Batch reward: 205.6, running avg: 340.387\n",
      "43: Avg Batch reward: 257.36, running avg: 338.457\n",
      "44: Avg Batch reward: 546.63, running avg: 343.188\n",
      "45: Avg Batch reward: 207.8, running avg: 340.179\n",
      "46: Avg Batch reward: 272.13, running avg: 338.7\n",
      "47: Avg Batch reward: 213.83, running avg: 336.043\n",
      "48: Avg Batch reward: 362.02, running avg: 336.584\n",
      "49: Avg Batch reward: 451.79, running avg: 338.935\n",
      "50: Avg Batch reward: 438.89, running avg: 340.934\n",
      "51: Avg Batch reward: 297.4, running avg: 340.081\n",
      "52: Avg Batch reward: 284.21, running avg: 339.006\n",
      "53: Avg Batch reward: 631.1, running avg: 344.518\n",
      "54: Avg Batch reward: 419.72, running avg: 345.91\n",
      "55: Avg Batch reward: 479.57, running avg: 348.34\n",
      "56: Avg Batch reward: 366.17, running avg: 348.659\n",
      "57: Avg Batch reward: 325.57, running avg: 348.254\n",
      "58: Avg Batch reward: 180.89, running avg: 345.368\n",
      "59: Avg Batch reward: 189.04, running avg: 342.718\n",
      "60: Avg Batch reward: 277.38, running avg: 341.63\n",
      "61: Avg Batch reward: 342.37, running avg: 341.642\n",
      "62: Avg Batch reward: 315.24, running avg: 341.216\n",
      "63: Avg Batch reward: 243.18, running avg: 339.66\n",
      "64: Avg Batch reward: 296.8, running avg: 338.99\n",
      "65: Avg Batch reward: 396.09, running avg: 339.868\n",
      "66: Avg Batch reward: 856.21, running avg: 347.692\n",
      "67: Avg Batch reward: 279.66, running avg: 346.676\n",
      "68: Avg Batch reward: 227.66, running avg: 344.926\n",
      "69: Avg Batch reward: 291.62, running avg: 344.154\n",
      "70: Avg Batch reward: 199.65, running avg: 342.089\n",
      "71: Avg Batch reward: 258.83, running avg: 340.917\n",
      "72: Avg Batch reward: 364.94, running avg: 341.25\n",
      "73: Avg Batch reward: 477.27, running avg: 343.114\n",
      "74: Avg Batch reward: 598.97, running avg: 346.571\n",
      "75: Avg Batch reward: 534.62, running avg: 349.078\n",
      "76: Avg Batch reward: 478.02, running avg: 350.775\n",
      "77: Avg Batch reward: 277.55, running avg: 349.824\n",
      "78: Avg Batch reward: 263.21, running avg: 348.714\n",
      "79: Avg Batch reward: 365.47, running avg: 348.926\n",
      "80: Avg Batch reward: 466.46, running avg: 350.395\n",
      "81: Avg Batch reward: 353.6, running avg: 350.434\n",
      "82: Avg Batch reward: 270.16, running avg: 349.455\n",
      "83: Avg Batch reward: 302.56, running avg: 348.89\n",
      "84: Avg Batch reward: 312.66, running avg: 348.459\n",
      "85: Avg Batch reward: 295.45, running avg: 347.836\n",
      "86: Avg Batch reward: 451.86, running avg: 349.045\n",
      "87: Avg Batch reward: 1169.1, running avg: 358.471\n",
      "88: Avg Batch reward: 2512.8, running avg: 382.952\n",
      "89: Avg Batch reward: 1779.0, running avg: 398.639\n",
      "90: Avg Batch reward: 522.13, running avg: 400.011\n",
      "91: Avg Batch reward: 1681.8, running avg: 414.097\n",
      "92: Avg Batch reward: 25.18, running avg: 409.87\n",
      "93: Avg Batch reward: 51.4, running avg: 406.015\n",
      "94: Avg Batch reward: 88.74, running avg: 402.64\n",
      "95: Avg Batch reward: 87.86, running avg: 399.327\n",
      "96: Avg Batch reward: 118.93, running avg: 396.406\n",
      "97: Avg Batch reward: 126.83, running avg: 393.627\n",
      "98: Avg Batch reward: 118.93, running avg: 390.824\n",
      "99: Avg Batch reward: 114.47, running avg: 388.032\n",
      "100: Avg Batch reward: 135.71, running avg: 385.509\n",
      "101: Avg Batch reward: 142.61, running avg: 383.104\n",
      "102: Avg Batch reward: 159.42, running avg: 380.911\n",
      "103: Avg Batch reward: 173.34, running avg: 378.896\n",
      "104: Avg Batch reward: 180.62, running avg: 376.989\n",
      "105: Avg Batch reward: 179.53, running avg: 375.109\n",
      "106: Avg Batch reward: 173.12, running avg: 373.203\n",
      "107: Avg Batch reward: 180.41, running avg: 371.401\n",
      "108: Avg Batch reward: 189.92, running avg: 369.721\n",
      "109: Avg Batch reward: 193.69, running avg: 368.106\n",
      "110: Avg Batch reward: 204.84, running avg: 366.622\n",
      "111: Avg Batch reward: 214.98, running avg: 365.256\n",
      "112: Avg Batch reward: 218.38, running avg: 363.944\n",
      "113: Avg Batch reward: 230.28, running avg: 362.761\n",
      "114: Avg Batch reward: 230.64, running avg: 361.602\n",
      "115: Avg Batch reward: 236.56, running avg: 360.515\n",
      "116: Avg Batch reward: 249.13, running avg: 359.555\n",
      "117: Avg Batch reward: 255.63, running avg: 358.667\n",
      "118: Avg Batch reward: 255.49, running avg: 357.792\n",
      "119: Avg Batch reward: 254.48, running avg: 356.924\n",
      "120: Avg Batch reward: 277.73, running avg: 356.264\n",
      "121: Avg Batch reward: 280.7, running avg: 355.64\n",
      "122: Avg Batch reward: 282.51, running avg: 355.04\n",
      "123: Avg Batch reward: 288.64, running avg: 354.5\n",
      "124: Avg Batch reward: 307.44, running avg: 354.121\n",
      "125: Avg Batch reward: 308.24, running avg: 353.754\n",
      "126: Avg Batch reward: 315.44, running avg: 353.45\n",
      "127: Avg Batch reward: 338.95, running avg: 353.336\n",
      "128: Avg Batch reward: 349.85, running avg: 353.308\n",
      "129: Avg Batch reward: 361.36, running avg: 353.371\n",
      "130: Avg Batch reward: 419.47, running avg: 353.879\n",
      "131: Avg Batch reward: 572.35, running avg: 355.547\n",
      "132: Avg Batch reward: 676.17, running avg: 357.976\n",
      "133: Avg Batch reward: 1648.1, running avg: 367.676\n",
      "134: Avg Batch reward: 259.34, running avg: 366.868\n",
      "135: Avg Batch reward: 255.93, running avg: 366.046\n",
      "136: Avg Batch reward: 259.63, running avg: 365.264\n",
      "137: Avg Batch reward: 252.34, running avg: 364.439\n",
      "138: Avg Batch reward: 292.82, running avg: 363.92\n",
      "139: Avg Batch reward: 239.32, running avg: 363.024\n",
      "140: Avg Batch reward: 281.06, running avg: 362.438\n",
      "141: Avg Batch reward: 295.94, running avg: 361.967\n",
      "142: Avg Batch reward: 346.96, running avg: 361.861\n",
      "143: Avg Batch reward: 398.15, running avg: 362.115\n",
      "144: Avg Batch reward: 442.88, running avg: 362.676\n",
      "145: Avg Batch reward: 401.37, running avg: 362.943\n",
      "146: Avg Batch reward: 492.41, running avg: 363.829\n",
      "147: Avg Batch reward: 404.4, running avg: 364.105\n",
      "148: Avg Batch reward: 386.44, running avg: 364.256\n",
      "149: Avg Batch reward: 611.41, running avg: 365.915\n",
      "150: Avg Batch reward: 2166.3, running avg: 377.918\n",
      "151: Avg Batch reward: 511.89, running avg: 378.805\n",
      "152: Avg Batch reward: 474.75, running avg: 379.436\n",
      "153: Avg Batch reward: 477.95, running avg: 380.08\n",
      "154: Avg Batch reward: 382.38, running avg: 380.095\n",
      "155: Avg Batch reward: 425.06, running avg: 380.385\n",
      "156: Avg Batch reward: 494.46, running avg: 381.116\n",
      "157: Avg Batch reward: 387.34, running avg: 381.156\n",
      "158: Avg Batch reward: 418.44, running avg: 381.392\n",
      "159: Avg Batch reward: 505.98, running avg: 382.175\n",
      "160: Avg Batch reward: 430.04, running avg: 382.475\n",
      "161: Avg Batch reward: 398.46, running avg: 382.574\n",
      "162: Avg Batch reward: 458.02, running avg: 383.04\n",
      "163: Avg Batch reward: 503.26, running avg: 383.777\n",
      "164: Avg Batch reward: 554.22, running avg: 384.816\n",
      "165: Avg Batch reward: 482.57, running avg: 385.409\n",
      "166: Avg Batch reward: 538.97, running avg: 386.334\n",
      "167: Avg Batch reward: 537.06, running avg: 387.236\n",
      "168: Avg Batch reward: 433.05, running avg: 387.509\n",
      "169: Avg Batch reward: 411.17, running avg: 387.649\n",
      "170: Avg Batch reward: 469.96, running avg: 388.133\n",
      "171: Avg Batch reward: 305.21, running avg: 387.648\n",
      "172: Avg Batch reward: 383.04, running avg: 387.622\n",
      "173: Avg Batch reward: 340.85, running avg: 387.351\n",
      "174: Avg Batch reward: 401.0, running avg: 387.43\n",
      "175: Avg Batch reward: 389.24, running avg: 387.44\n",
      "176: Avg Batch reward: 460.13, running avg: 387.853\n",
      "177: Avg Batch reward: 425.73, running avg: 388.067\n",
      "178: Avg Batch reward: 462.78, running avg: 388.487\n",
      "179: Avg Batch reward: 457.0, running avg: 388.87\n",
      "180: Avg Batch reward: 518.4, running avg: 389.589\n",
      "181: Avg Batch reward: 425.58, running avg: 389.788\n",
      "182: Avg Batch reward: 375.88, running avg: 389.712\n",
      "183: Avg Batch reward: 381.7, running avg: 389.668\n",
      "184: Avg Batch reward: 456.14, running avg: 390.029\n",
      "185: Avg Batch reward: 491.45, running avg: 390.577\n",
      "186: Avg Batch reward: 501.69, running avg: 391.175\n",
      "187: Avg Batch reward: 499.87, running avg: 391.756\n",
      "188: Avg Batch reward: 487.67, running avg: 392.266\n",
      "189: Avg Batch reward: 420.66, running avg: 392.416\n",
      "190: Avg Batch reward: 495.86, running avg: 392.961\n",
      "191: Avg Batch reward: 625.19, running avg: 394.177\n",
      "192: Avg Batch reward: 612.45, running avg: 395.313\n",
      "193: Avg Batch reward: 564.42, running avg: 396.19\n",
      "194: Avg Batch reward: 748.69, running avg: 398.007\n",
      "195: Avg Batch reward: 909.77, running avg: 400.631\n",
      "196: Avg Batch reward: 5000.0, running avg: 424.097\n",
      "197: Avg Batch reward: 4978.8, running avg: 447.218\n",
      "198: Avg Batch reward: 2019.2, running avg: 455.157\n",
      "199: Avg Batch reward: 1129.5, running avg: 458.545\n",
      "200: Avg Batch reward: 456.31, running avg: 458.534\n",
      "201: Avg Batch reward: 441.27, running avg: 458.448\n",
      "202: Avg Batch reward: 392.51, running avg: 458.122\n",
      "203: Avg Batch reward: 405.42, running avg: 457.862\n",
      "204: Avg Batch reward: 454.13, running avg: 457.844\n",
      "205: Avg Batch reward: 468.96, running avg: 457.898\n",
      "206: Avg Batch reward: 454.4, running avg: 457.881\n",
      "207: Avg Batch reward: 465.26, running avg: 457.917\n",
      "208: Avg Batch reward: 392.91, running avg: 457.604\n",
      "209: Avg Batch reward: 380.97, running avg: 457.238\n",
      "210: Avg Batch reward: 396.96, running avg: 456.951\n",
      "211: Avg Batch reward: 454.65, running avg: 456.94\n",
      "212: Avg Batch reward: 390.92, running avg: 456.628\n",
      "213: Avg Batch reward: 555.66, running avg: 457.093\n",
      "214: Avg Batch reward: 1083.0, running avg: 460.018\n",
      "215: Avg Batch reward: 597.71, running avg: 460.658\n",
      "216: Avg Batch reward: 609.18, running avg: 461.346\n",
      "217: Avg Batch reward: 508.26, running avg: 461.562\n",
      "218: Avg Batch reward: 503.21, running avg: 461.753\n",
      "219: Avg Batch reward: 562.23, running avg: 462.212\n",
      "220: Avg Batch reward: 607.39, running avg: 462.872\n",
      "221: Avg Batch reward: 626.94, running avg: 463.614\n",
      "222: Avg Batch reward: 498.94, running avg: 463.773\n",
      "223: Avg Batch reward: 508.73, running avg: 463.975\n",
      "224: Avg Batch reward: 554.03, running avg: 464.377\n",
      "225: Avg Batch reward: 468.94, running avg: 464.397\n",
      "226: Avg Batch reward: 450.28, running avg: 464.335\n",
      "227: Avg Batch reward: 420.05, running avg: 464.14\n",
      "228: Avg Batch reward: 430.82, running avg: 463.994\n",
      "229: Avg Batch reward: 554.98, running avg: 464.391\n",
      "230: Avg Batch reward: 447.25, running avg: 464.316\n",
      "231: Avg Batch reward: 474.53, running avg: 464.361\n",
      "232: Avg Batch reward: 594.76, running avg: 464.923\n",
      "232: Avg Batch reward: 594.76, running avg: 464.923\n",
      "233: Avg Batch reward: 696.82, running avg: 465.918\n",
      "234: Avg Batch reward: 713.21, running avg: 466.975\n",
      "235: Avg Batch reward: 624.18, running avg: 467.644\n",
      "236: Avg Batch reward: 742.63, running avg: 468.809\n",
      "237: Avg Batch reward: 603.42, running avg: 469.377\n",
      "238: Avg Batch reward: 559.77, running avg: 469.757\n",
      "239: Avg Batch reward: 483.49, running avg: 469.814\n",
      "240: Avg Batch reward: 490.12, running avg: 469.899\n",
      "241: Avg Batch reward: 433.82, running avg: 469.749\n",
      "242: Avg Batch reward: 439.27, running avg: 469.623\n",
      "243: Avg Batch reward: 384.25, running avg: 469.272\n",
      "Stopping Looping!\n",
      "Interrupted loop (244): interrupted episode reward: 915.0, info: {}\n",
      "Num Batches: 245, Avg Reward: 46709.522449\n",
      "Final Weights: [ -37.68294777   51.80612897  110.52220448  167.58712393]\n"
     ]
    }
   ],
   "source": [
    "# Start by printing any previous runs so you can start & stop w/out losing\n",
    "# output history.\n",
    "for batch_number in range(len(batch_rewards)):\n",
    "    print(\"{0}: Avg Batch reward: {1:.5}, running avg: {2:.6}\".format(batch_number, batch_rewards[batch_number], running_avg_rewards[batch_number]))\n",
    "try:\n",
    "    while True:\n",
    "        gradient = np.zeros_like(W)\n",
    "        total_batch_reward = 0.0\n",
    "        for ep in range(0,batch_size):\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            total_episode_reward = 0\n",
    "            observations = []\n",
    "            rewards = []\n",
    "            y_probs = []\n",
    "            actions_taken = []\n",
    "            for _ in range(max_episode_length):\n",
    "                #env.render()\n",
    "                action, y = model_forward_step(W,observation)\n",
    "                observations.append(observation)\n",
    "                y_probs.append(y)\n",
    "                actions_taken.append(action)\n",
    "\n",
    "                observation, reward, done, info = env.step(action)\n",
    "\n",
    "                rewards.append(reward)\n",
    "                total_episode_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # End of the Episode\n",
    "            episode_number += 1\n",
    "            discounted_ep_rewards = discount_rewards(rewards, discount_factor)\n",
    "            # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            discounted_ep_rewards -= np.mean(discounted_ep_rewards)\n",
    "            discounted_ep_rewards /= np.std(discounted_ep_rewards)\n",
    "\n",
    "            ep_grad = np.zeros_like(W)\n",
    "            for i in range(0, len(observations)):\n",
    "                ep_grad += model_backward_step(observations[i],\n",
    "                                               y_probs[i],\n",
    "                                               actions_taken[i],\n",
    "                                               discounted_ep_rewards[i])\n",
    "\n",
    "            gradient += ep_grad\n",
    "            total_batch_reward += total_episode_reward\n",
    "\n",
    "        # End of batch\n",
    "        Ws.append(np.copy(W))\n",
    "        total_reward += total_batch_reward\n",
    "        running_avg_reward = total_reward/((batch_number+1)*batch_size)\n",
    "        batch_rewards.append(total_batch_reward/batch_size)\n",
    "        running_avg_rewards.append(running_avg_reward)\n",
    "\n",
    "        W += learning_rate * gradient\n",
    "        gradient = np.zeros_like(W) # reset batch gradient buffer\n",
    "        \n",
    "        if (batch_number % 1) == 0:\n",
    "            print(\"{0}: Avg Batch reward: {1:.5}, running avg: {2:.6}\".format(batch_number, batch_rewards[batch_number], running_avg_rewards[batch_number]))\n",
    "\n",
    "        batch_number += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping Looping!\")\n",
    "    print(\"Interrupted loop ({0}): interrupted episode reward: {1:.5}, info: {2}\".format(batch_number, total_episode_reward, info))\n",
    "\n",
    "num_batches = batch_number+1\n",
    "average_reward = total_reward/num_batches\n",
    "print(\"Num Batches: {0}, Avg Reward: {1}\".format(num_batches,average_reward))\n",
    "print(\"Final Weights:\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   9.49131137,   41.10327388,   79.00562113,  105.23269497])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -0.70131069,  70.63520384,  49.07602493,  85.43034609]),\n",
       " array([  1.88725698,  83.63077012,  49.98254472,  85.9457884 ]),\n",
       " array([   2.43431074,   21.19758587,   66.19355127,  119.96909819]),\n",
       " array([   6.63932667,   46.83325411,   76.73963084,  125.92736569]),\n",
       " array([   7.30117878,   56.26146235,   78.48161558,  121.01161769]),\n",
       " array([   6.59378345,   60.26226121,   79.43918747,  119.39795227]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ws[140], Ws[146], Ws[158], Ws[182], Ws[193], Ws[201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_saved_Ws, _saved_batch_rewards, _saved_running_avg_rewards = Ws, batch_rewards, running_avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_model(W, num_steps=max_episode_length*2, num_test_episodes = 5):\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i_episode in range(num_test_episodes):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        print(\"{0}/{1}:\".format(i_episode, num_test_episodes))\n",
    "        for _ in range(num_steps):\n",
    "            env.render()  # I don't think you can get this to render from MyBinder. :(\n",
    "            action,_ = model_forward_step(W,observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(\"{0}/{1}: Episode Reward: {2}\".format(i_episode, num_test_episodes, episode_reward))\n",
    "        total_reward += episode_reward\n",
    "    average_reward = total_reward/num_test_episodes\n",
    "    return total_reward,average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3:\n",
      "0/3: Episode Reward: 16750.0\n",
      "1/3:\n",
      "1/3: Episode Reward: 14742.0\n",
      "2/3:\n",
      "2/3: Episode Reward: 18980.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50472.0, 16824.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(Ws[140], num_steps=max_episode_length*10, num_test_episodes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1:\n",
      "0/1: Episode Reward: 50000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000.0, 50000.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(Ws[158], max_episode_length*10, num_test_episodes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/2:\n",
      "0/2: Episode Reward: 10000.0\n",
      "1/2:\n",
      "1/2: Episode Reward: 10000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20000.0, 10000.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(personal_best_weight, num_test_episodes = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Observations\n",
    "- Often the model will run for a long time at a very low initial reward and not improve dozens of batches. I assume this is because it happens to start off in a bad part of the weight space and struggles to find any improvements.\n",
    "- Also, almost always while training the model will make very saw-toothed improvements, getting better and better until it very drastically drops back down to ~200 or 300. Sometimes it even hits the max runs per episode, but then the very next update brings it back down. I don't know why this happens, but it still seems to *eventually* converge (as far as I can tell).\n",
    "- For example of the above, in one run I saw this:\n",
    "```\n",
    "135: Avg Batch reward: 349.95, running avg: 135.458\n",
    "136: Avg Batch reward: 334.93, running avg: 136.914\n",
    "137: Avg Batch reward: 486.63, running avg: 139.448\n",
    "138: Avg Batch reward: 702.71, running avg: 143.5\n",
    "139: Avg Batch reward: 744.29, running avg: 147.791\n",
    "140: Avg Batch reward: 5000.0, running avg: 182.204\n",
    "141: Avg Batch reward: 4162.8, running avg: 210.236\n",
    "142: Avg Batch reward: 762.46, running avg: 214.098\n",
    "143: Avg Batch reward: 384.87, running avg: 215.284\n",
    "144: Avg Batch reward: 645.32, running avg: 218.25\n",
    "```\n",
    "- Sometimes I'll see it come and go from `5000.0` like above. Does this mean my learning rate ($\\alpha$) is too large?\n",
    "```\n",
    "31: Avg Batch reward: 487.95, running avg: 341.933\n",
    "32: Avg Batch reward: 265.17, running avg: 339.607\n",
    "33: Avg Batch reward: 5000.0, running avg: 476.677\n",
    "34: Avg Batch reward: 5000.0, running avg: 605.915\n",
    "35: Avg Batch reward: 3584.6, running avg: 688.657\n",
    "36: Avg Batch reward: 153.38, running avg: 674.19\n",
    "37: Avg Batch reward: 169.31, running avg: 660.903\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
