{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data!\n",
    "------------\n",
    "Shakespeare's comedies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 2108622 characters, 69 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('all_comedies_cat.txt', 'r').read() # should be simple plain text file\n",
    "#data = \"abcdefghijkabcdefghijk\"\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_hot_vec(size, t):\n",
    "    ''' size: a list of dimensions, like is passed to np functions.\n",
    "        t:    the index to set to 1 (the truth).\n",
    "    '''\n",
    "    xs = np.zeros(size) # encode in 1-of-k representation\n",
    "    xs[0,t] = 1\n",
    "    return xs\n",
    "make_hot_vec([1,4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "num_steps = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------\n",
    "Define the Model.\n",
    "------\n",
    "This is modelled after Karpathy's minimal RNN, described in his [excellent article](karpathy.github.io/2015/05/21/rnn-effectiveness/) and implemented in this [short gist](https://gist.github.com/karpathy/d4dee566867f8291f086). The basic step function is:\n",
    "\n",
    "```\n",
    "# update the hidden state\n",
    "self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "# compute the output vector\n",
    "y = np.dot(self.W_hy, self.h)\n",
    "```\n",
    "\n",
    "There are also two biases, `bh` and `by`. So the model has 5 learned parameters: `Wxh`, `Whh`, `Why`, `bh`, and `by`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a single step in the RNN from one char to the next\n",
    "Wxh = tf.Variable(tf.random_uniform([vocab_size, hidden_size], maxval=0.01)) \n",
    "Whh = tf.Variable(tf.random_uniform([hidden_size, hidden_size], maxval=0.01))\n",
    "bh = tf.Variable(tf.zeros([hidden_size]))\n",
    "Why = tf.Variable(tf.random_uniform([hidden_size, vocab_size], maxval=0.01)) \n",
    "by = tf.Variable(tf.zeros([vocab_size])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###Set up the recurrence\n",
    "\n",
    "This is important: A recurrent neural net is *recurrent* because the simple model is repeated multiple times to create the overall model. This is called \"unrolling\" the net.\n",
    "\n",
    "This matters, because in order for backpropogation to calculate the effect that the hidden state has on the loss, it needs to take the changes from one `state` to another into account when calculating the gradient. If you didn't unroll the network, and instead just returned the `state` from each run and passed the `state` back into the next run, the `state`'s only effect on the cost would be how the `state` effected `y`, not how the current `state` effected the next state.\n",
    "\n",
    "For example, the simple network without unrolling it looks like this:\n",
    "  \n",
    "$  cost = truth - y $, where $ y = W_{hy}*h' $ and $h' = tanh(W_{hh}*h + W_{xh}*x) $\n",
    "\n",
    "The current state, $h$, only effects $cost$ through it's impact on $y$. Even though it sets the *next state*, $h'$, the transition from $h$ to $h'$ is never considered during backpropogation.\n",
    "\n",
    "Instead, an *unrolled* network *does* effect the cost both from the current $h$ and on its effect on the next $h$:\n",
    "\n",
    "$  cost = (truth_1 - y_1) + (truth_0 - y_0) $\n",
    "\n",
    "for $ y_1 = W_{hy}*h_1 $, $h_1 = tanh(W_{hh}*h_0 + W_{xh}*x_1) $\n",
    "\n",
    "and\n",
    "$ y_0 = W_{hy}*h_0 $, $h_0 = tanh(W_{hh}*h + W_{xh}*x_0) $\n",
    "\n",
    "Now, $W_{hh}$'s impact on the next $h$ effects the cost just as much as its impact on the current $y$.\n",
    "\n",
    "SO, I *think*, the more steps you unroll a network for, the more emphasis you're placing on the hidden state's impact versus the other weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=TensorShape([Dimension(None), Dimension(69)]), dtype=float32) Tensor(\"Placeholder_1:0\", shape=TensorShape([Dimension(None), Dimension(69)]), dtype=float32) Tensor(\"Placeholder_25:0\", shape=TensorShape([Dimension(None), Dimension(100)]), dtype=float32) Tensor(\"Tanh:0\", shape=TensorShape([Dimension(None), Dimension(100)]), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inputs = [tf.placeholder(tf.float32, [None, vocab_size]) for _ in xrange(num_steps)]\n",
    "outputs = {}\n",
    "hs = {}\n",
    "hs[-1] = tf.placeholder(tf.float32, [None, hidden_size])\n",
    "for i in range(len(inputs)):\n",
    "    hs[i]      = tf.nn.tanh(tf.matmul(inputs[i], Wxh) + tf.matmul(hs[i-1], Whh) + bh)\n",
    "    outputs[i] = tf.nn.softmax(tf.matmul(hs[i], Why) + by)\n",
    "print inputs[0], inputs[1], hs[-1], hs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# Variables must be initialized by running an `init` Op after having\n",
    "# launched the graph.  We first have to add the `init` Op to the graph.\n",
    "init_op = tf.initialize_all_variables()\n",
    "# Run the 'init' op\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----\n",
    "Training\n",
    "-------\n",
    "\n",
    "This is the \"cross-entropy\" algorithm.\n",
    "$$\n",
    "H_{y'}(y)= âˆ’\\sum_i y_i' \\log(y_i)\n",
    "$$\n",
    " $-\\log(y_i)$ is useful because it turns percentages into (0,$\\infty$) \"cost\".\n",
    "So then I think multiplying by the truth cancels out the percentages for everything else, and summing just turns the one_hot vector into a value?\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement Cross Entropy:\n",
    "truths = [tf.placeholder(tf.float32, [None, vocab_size]) for _ in xrange(num_steps)]\n",
    "cross_entropies = [-tf.reduce_sum(truths[i]*tf.log(outputs[i])) for i in xrange(num_steps)]  # These operations act element-wise.\n",
    "cross_entropy = tf.add_n(cross_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This was big. I changed the optimizer to the AdagradOptimizer, and upped the learning\n",
    "#  rate from 0.01 to 0.1.\n",
    "# I think it's okay to increase the learning rate because Adagrad decreases the rate over time.\n",
    "train_step = tf.train.AdagradOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Do the thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##Hallucinating! The fun part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hallucinate(sess, seed_ix):\n",
    "    ix = seed_ix\n",
    "    ixes = [ix]\n",
    "    hallucination_h_state = np.zeros([1, hidden_size]) \n",
    "    for i in range(100):\n",
    "        # Create a random starting letter \n",
    "        x_in = make_hot_vec([1,vocab_size], ix)\n",
    "\n",
    "        feed_dict={inputs[0]: x_in}\n",
    "        feed_dict.update({hs[-1]:hallucination_h_state})\n",
    "\n",
    "        output, hallucination_h_state = sess.run((outputs[0],hs[0]), feed_dict=feed_dict)\n",
    "        probs = output[0]\n",
    "        ix = np.random.choice(range(len(probs)), p=probs)\n",
    "        ixes.append(ix)\n",
    "    hallucination=''.join([ix_to_char[ix] for ix in ixes])\n",
    "    print hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "iterations = 0\n",
    "ix = 0\n",
    "h_state = np.zeros([1, hidden_size])\n",
    "smooth_loss = -np.log(1.0/vocab_size)*num_steps # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Iteration  1  Loss:  104.666082071  ==============\n",
      "]D:oMoRRRRoRo ttoRooooioooBMnooooWooAuDou BtRuRoRno\n",
      "RtniBoouRoRn\n",
      "Ro RoURootoRooo\tRoBnRBREuTooBuMtoCoo\n",
      "========== Iteration  2  Loss:  102.148146455  ==============\n",
      "WE\tfnea  e  ha Co   e |sos u  tCv aoo  n  s  u  ooeo r\n",
      "o ot ur  aoovC e e  uu s e    v.aeth  eaevonne\n",
      "========== Iteration  3  Loss:  99.4573122906  ==============\n",
      "rNN\n",
      "TECCHL\n",
      "CFENE:\n",
      "NE(LCN)NTEr) TS)SELEN\n",
      "aE(HrSTT:\n",
      " A. T\n",
      ")OCTaTN)mm)NNtCOUae\n",
      "Em\n",
      "TENH)LE\n",
      "SCtNNaTrEU)\n",
      "(C\n",
      "========== Iteration  4  Loss:  97.2320978114  ==============\n",
      "BN\tA \tId A)\n",
      "oAItAANoAh\n",
      " tWttA toNtI)DAA\n",
      " ttN\tAAoItt)u AID  \to\n",
      "t  Ah\t:ADN)) :)otAWAAA\tooA\n",
      "t\n",
      "\n",
      "\t\n",
      "Ate   e\n",
      "========== Iteration  5  Loss:  95.0463202377  ==============\n",
      "TDoeiOrsOosd\n",
      "oeddfssIrOdsroLLssie  esdfeLscsdcdSfrdsfOOsds\n",
      "eL\t\tf\n",
      "\toe ileiiOrfe c sssdfTo dw ssLr\n",
      "fsfL\n",
      "========== Iteration  6  Loss:  92.5321700427  ==============\n",
      "g\n",
      "\n",
      "\n",
      "inGGF)un\n",
      "sl\tG)lels)\n",
      "m)\tere\n",
      "d\n",
      ")e:iln) Fl\ttrtt(n\t:FS)o\n",
      "\n",
      "\ttFsGna:)(shVo):((ne)a)Lei\tieS\n",
      "GF\tmGt\t\tn\n",
      ")D\n",
      "========== Iteration  7  Loss:  89.8159724789  ==============\n",
      "]tlansnlelesrt Fs(rsilPFlcs rlorc rPeceneislsis;a es;l.ar;rerl;l;rselerererlsi; ;;lenionice  cMllsinc\n",
      "========== Iteration  8  Loss:  87.4253396397  ==============\n",
      "[sAcElEE\tnt\n",
      "\t[\n",
      "\n",
      "ReT\n",
      "RtRttcen [\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " a E AR E[[M\n",
      "a BacN\n",
      "\n",
      "\tlEBMtlR \n",
      "\ttE\t[EtRR\taB\n",
      "ME \n",
      "\n",
      "[areETRR E\n",
      "\n",
      "R \n",
      "T[\n",
      "========== Iteration  9  Loss:  85.607773481  ==============\n",
      "; rdovnv ymmmd  mrv   T l\tmm reli iiiie oro imomlriloon\t  y\n",
      "\n",
      "vme iniTiirm\t vverordyonsrm   molyi  s s\n",
      "========== Iteration  10  Loss:  83.3564158302  ==============\n",
      "Gt mg  w::w\t\t 'Oar\tuhbtua\ta:wT  awd\n",
      "\n",
      "\n",
      "e h  \t's\n",
      "\n",
      "\n",
      "\n",
      "ueneau'\n",
      "\n",
      "I :au h\tAde e\t:mw\n",
      "\n",
      "\n",
      "'awI\tw \tasd'\tan a w\n",
      "\ta\n",
      "========== Iteration  11  Loss:  81.3029723492  ==============\n",
      "STELjAULLonuniwocjEEeL.uus\n",
      "U iuYY\t FuieUwUFAo.\tYLwUsnE\n",
      "\tLj\tYc\n",
      "E\t.t FFii\n",
      "EA.\tAr\n",
      "dueYAh UE ch.\n",
      "LijcenN\t\n",
      "========== Iteration  12  Loss:  79.0626323853  ==============\n",
      ")\talo \tymliio\ttsyt   melammilemstlytsie \talg yit iyyt t ys gii oielg, \n",
      " r ty li Riisiimloroi seile si\n",
      "========== Iteration  13  Loss:  76.7442208967  ==============\n",
      "S\t tiwhawedd wtetheru.ainheru wh neuhap we re pt ratltnt wehe heth wadenert wa wdewondune darr wt tth\n",
      "========== Iteration  14  Loss:  74.4712067798  ==============\n",
      "ir'dyinyoiy med jefthmdesCilds's isdtsmetf reh'md'nasjsof idpam'ntmast s'imom handmifff m atdytmae me\n",
      "========== Iteration  15  Loss:  72.7112688232  ==============\n",
      ",  arsice tiee ae teetate tu dersece hd t itee ie ct uh daha ie Ee du trdtttheashts hetii \n",
      "aEe teed  \n",
      "========== Iteration  16  Loss:  70.9377262854  ==============\n",
      ")mOos\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "of OheyeUC\n",
      "\ttgo hOpn.\n",
      "\n",
      "ytyefe\n",
      "\n",
      "\n",
      "\n",
      "TitC bm.\n",
      "\n",
      " tre.ilaff.O. \n",
      "\n",
      " bgyt,e. fobpy \n",
      "\n",
      "\n",
      "hbUelobomine \n",
      "========== Iteration  17  Loss:  69.4676296439  ==============\n",
      ":\tihs!l-- waltslkililisalohawkllal- olisowiwl-lkhllsh-sl wo-!dl! wsl sisilasltwllldlw\n",
      "- l, w!loshsk !\n",
      "========== Iteration  18  Loss:  67.3673863794  ==============\n",
      "bGeatm hrimaat at,\n",
      "aare,,aorair ind \n",
      "alondarthtd tameldeeno\n",
      "ime h m\n",
      "\n",
      "mond aoralmetmod, lanmamimed hat\n",
      "========== Iteration  19  Loss:  65.44757377  ==============\n",
      "ufe !outheune Il ve linI ih vand hond we hiv!mind hkk Ih wh I! tueuutu thinon ot ninat wt tntnIgonk w\n",
      "========== Iteration  20  Loss:  63.9742603325  ==============\n",
      "w\tH??H?STC?Nad,ama\n",
      "\n",
      "Nadde \n",
      ",Ee kN\tC,eU aU?CSTdeS wav m?N\n",
      "EadEas?Sas, mg\n",
      "\n",
      "\n",
      "\ttSSad\n",
      "\n",
      "CTHeako,CS\n",
      "TaU\n",
      "\tsf?\n",
      "========== Iteration  21  Loss:  62.2229466194  ==============\n",
      "n: AN\n",
      "bbd\taonirarNAnUNw warTN:dLA\n",
      "\tUe\n",
      "\tAd Ueand\tad \n",
      "\tAd.\tondddat\n",
      "LbeHeFrar\n",
      "\tYdade.\n",
      "Ear\n",
      "\tHen \n",
      "Aonarera\n",
      "========== Iteration  22  Loss:  60.7492102548  ==============\n",
      "ld,\n",
      "hand\n",
      "\twon rasen\n",
      "\tand rure and\n",
      "\n",
      "homund\n",
      "\tngnmury as hengd: osamly wanangay \n",
      "\n",
      "\thndyHang wou:ny: theu\n",
      "========== Iteration  23  Loss:  60.5101798549  ==============\n",
      " Ralt\n",
      "\n",
      "\til\n",
      "\n",
      "Ra.\tATEgEW AM\n",
      "NRiriRt\n",
      "\tty.\n",
      "\tlafa.\n",
      "\n",
      "TRt\n",
      "\tal.MleMathaf ta.\n",
      "yorRTEyisoRAMEanw h.\tTitaBat aA \n",
      "========== Iteration  24  Loss:  59.4054380596  ==============\n",
      "z\n",
      "\tand fe bFEafe hofAhe berif aot Iarof fere \n",
      " ont oM\t\n",
      "\tARIRfeatEeny MM\n",
      "\tfrfeotlhorstTas Af\tI\tatarRod\n",
      "========== Iteration  25  Loss:  57.9886743753  ==============\n",
      ")\n",
      "\n",
      "of de of\tG d \n",
      "fe mar herderFanor ooNd Gef ne darerdenNrad nd Narf bad ne t os barre earpar de Nat \n",
      "========== Iteration  26  Loss:  56.7172592243  ==============\n",
      "M\tho eh o koset os poro ho sgoges phevot ope hhof sgo oo sos soro ohe sove he rg thos ho s posot th t\n",
      "========== Iteration  27  Loss:  56.0764898034  ==============\n",
      "S\n",
      "\t;r ghe t s tfsgir ffgrrrife her fhir. st wrirg gfrirgir itr ffi; kaworirrger w;ra; fafr; itrrarrir\n",
      "========== Iteration  28  Loss:  55.5104780708  ==============\n",
      "xsiep theiue vas; pertper r;\tEitiiy theye thes iu;yeyeatpeveirytither tha vipe yiiy ras peyhereisint \n",
      "========== Iteration  29  Loss:  54.1201032657  ==============\n",
      "\n",
      "\therind as haly\n",
      "\tes neriny ah sene hindirsonivene hes hyhoctye thoneavenerinyhineinerv; hihed \the ha\n",
      "========== Iteration  30  Loss:  53.7562621616  ==============\n",
      "zitOhertTEN'ir TOtT.TEsT's bubs 'iE! mhe Nhe U\t'b'TSess Nt'STEnNNSisb casrU bUTEN\tt\n",
      "C\tNsTtSC\tUb mbors\n",
      "========== Iteration  31  Loss:  53.2156331494  ==============\n",
      "of heatnt\n",
      "bpchappharbtphaheareaciathahear hache coos heactars cp therathe\n",
      "\tas coeracer hereaos\n",
      "\n",
      "\taaro\n",
      "========== Iteration  32  Loss:  52.77911229  ==============\n",
      "zHos, feH,\n",
      "\tHaH\n",
      "\tHe thos,, ;,ed wiong, chen;,of\n",
      "\tHN,einlito,en\t\te, on\n",
      "\tofs, of, f, oo \n",
      "ale Nf\ttf fitt\n",
      "========== Iteration  33  Loss:  52.8328949162  ==============\n",
      "PSosNiw AfEas. Af Lt co tfeond cor corad vo sf\n",
      "\tANawLo af Icofad wo IE\tU veruce. Lcond; Io foE I\tEoco\n",
      "========== Iteration  34  Loss:  52.5357535753  ==============\n",
      "hesixdere,\n",
      "\tig, rif\teace;,sgiifec,\n",
      "\titsis\n",
      "Cet xfd iif d.\tfiingge\n",
      "\tagheis,siededes gx\n",
      "\tais,\tefees acfi\n",
      "========== Iteration  35  Loss:  51.5502545974  ==============\n",
      "e\n",
      "\ttomos so\n",
      "\toamg\n",
      "\tioshere geM,oons be somkses acm\n",
      "\tt\n",
      "\tt Thecee make to\n",
      "\tof tosxg. toat tcs\n",
      "\t\ther of \n",
      "========== Iteration  36  Loss:  50.862099838  ==============\n",
      "ZNoSSSSet?NANTE,\n",
      "ES\tOUNontN\n",
      "\thOtont?\n",
      "NTEthou tFEUOES\tTEe nt\ttNCUhed?BEU Ot SSTEw?\n",
      "COs? TESaut\n",
      "\tTEthe \n",
      "========== Iteration  37  Loss:  50.3770921368  ==============\n",
      ")\n",
      "\n",
      "OAtoeree te te\n",
      "\tCof tt\n",
      "\n",
      "C\ther, maund\n",
      "\ttutuen Cbisre\n",
      "\tintat Cind?\n",
      "riniererndF\n",
      "CC\n",
      "\tWind Core\n",
      "\n",
      "\n",
      "COrut\n",
      "========== Iteration  38  Loss:  50.3015335978  ==============\n",
      "?\n",
      "\tane tl be bC\n",
      "\n",
      "\tonen toonDe ger ronDe ong to bo! le ve h gent fng anoeo g deinD ge y wonw nnorong a\n",
      "========== Iteration  39  Loss:  49.792958273  ==============\n",
      "jerdkon feisk be ee nnern qhy yefe': ihy fey\n",
      "\t'e:: fhy ley thy thow: honee shy hiows:' wesse hoon hoo\n",
      "========== Iteration  40  Loss:  49.5726632822  ==============\n",
      "alher mive thh ht maaviltuamtuthy hei her Thats ihy\n",
      "\teu  fer ane wun thanim, thy hBas thy hees Wuruat\n",
      "========== Iteration  41  Loss:  49.576144095  ==============\n",
      "sisidd lor'iucourtouend'o te'tiaci moamird e'ergudoord ulyory \trumiirey aric;; goud; uorcortontirygoo\n",
      "========== Iteration  42  Loss:  49.066066267  ==============\n",
      "b ThisSale phad \n",
      "\tilve SOile OU tl\n",
      " sh:m Hhote thinN\n",
      "\n",
      "\tHirvavS Sher theeld CCiTs Thas COharo  NN ale \n",
      "========== Iteration  43  Loss:  48.7325101195  ==============\n",
      "y thas be thatcthes Nher\n",
      "\tat ches becat nt chatRAShe beat sT\tse\n",
      "\n",
      "b sasm chB be bofctatf Te c[n LAathe\n",
      "========== Iteration  44  Loss:  48.4583137915  ==============\n",
      "P\tuRacc,\n",
      "\tind meatheut nt mhersasmashes t ce moshork caud, maua mhar mt kass mes, anktham moshaa d me\n",
      "========== Iteration  45  Loss:  48.420530855  ==============\n",
      "A\n",
      "\tAaEfxtxRTEfad BxR\n",
      "\t[ant annd\n",
      "\tEE c[unt.\n",
      "\t[aRTEMt Tunt Bhe\n",
      "\n",
      "\n",
      "BEn.\n",
      "\n",
      "\n",
      "\t[anE\n",
      "\n",
      "\n",
      "BErt.\n",
      "\tAanr \n",
      "aufun Mx[t\n",
      "========== Iteration  46  Loss:  47.7297912322  ==============\n",
      ", Aacer.\n",
      "\tar\n",
      "\tAreredererere gime graseseseaarere tuasher maas geas mi mhes Arereres geseay irr cis ha\n",
      "========== Iteration  47  Loss:  46.8820478699  ==============\n",
      "verion tC\n",
      "ars goithim nart yoriman: gigt ition\n",
      "\tiranaririnong fn mamy CCanmningtrinigirn girnon manie\n",
      "========== Iteration  48  Loss:  46.5370600755  ==============\n",
      "lR\n",
      "B'b 'T byatrar be 'n bane wamof b's'mTf' wory al fenat waalatetavenawyory inan'ne byabe 'T amwov'f\n",
      "========== Iteration  49  Loss:  46.6045527892  ==============\n",
      "K\n",
      "\tI\n",
      "\tI\n",
      "\tI soor\n",
      "\tom i  hevibe ibm bovesimasant:b be b hem so b: iss lokes: bovemaso ho b\ttham: Ge is \n",
      "========== Iteration  50  Loss:  46.7384936813  ==============\n",
      "s ne ge.\tman ib an beme s yon matp ige ne.\n",
      "\tTinan SM\tTChamhein inin b ond se ] movive bn\n",
      "\n",
      "phan\n",
      "\tTun \n",
      "\n",
      "========== Iteration  51  Loss:  46.1317879155  ==============\n",
      "I povevt, fort.s aft wpovevttt MTve.\t''ot loverr,.\n",
      "\tTrt thtre thelevew\n",
      "\tThe.womtt sre poe.y lop d't.\n",
      "\n",
      "========== Iteration  52  Loss:  45.7931269283  ==============\n",
      "OWhaus,,\n",
      "\tHik,\n",
      "\tTreek,\thouwaki, ang gioway Hisla, chem, iny ags cine bews shiwiy hiskiwis hysins gisl\n",
      "========== Iteration  53  Loss:  45.2384486699  ==============\n",
      "\tThe hos nis fres wTheres hite hive hin facee: \n",
      "\tBere fus favent onw fues:\n",
      "\thour hit s sunrow\n",
      "\tBotet \n",
      "========== Iteration  54  Loss:  44.6339162818  ==============\n",
      "xKit mOEw.\n",
      "\tEore sL as WoRAL[E:rire.\n",
      "\n",
      "\t[Er LAPEores ]aRhtrhtue? yPRAPRA\n",
      "E S\n",
      "OALOy LAmd OA] honet PLon\n",
      "========== Iteration  55  Loss:  43.6867861138  ==============\n",
      "wEour ninos, a sinlivis mit din lour wishiatokiris init areond him akio san hid anoos fildias irs ono\n",
      "========== Iteration  56  Loss:  42.5823853597  ==============\n",
      "\n",
      "\tThat thes fam in hit thavith e totat thyy Whewt tht ll fimo; t thiy they theat in hit thwy athielio\n",
      "========== Iteration  57  Loss:  41.7869940351  ==============\n",
      ")\tisld waolsstowofwoud we: held wewed withel,\n",
      "\tCof w ad ide dime:\n",
      "\tCatowes wifred\n",
      "\tChe;\n",
      "\tCold\n",
      "\tons: w\n",
      "========== Iteration  58  Loss:  41.7286072474  ==============\n",
      "erS\n",
      "\n",
      "EETE\n",
      "\n",
      "\n",
      "OPOEPy! Pon, Pooum, monamy con,,\n",
      "\n",
      "Shan!imo cS\n",
      "\n",
      "\n",
      "\n",
      "LPL\tfor, qon, rou! ronas,\n",
      "\n",
      "\tAfOyofcN!\n",
      "\tH\n",
      "========== Iteration  59  Loss:  40.5026042873  ==============\n",
      "d evis some stiin saue hie tove sose sto seagile s.aEnionimevite s same s ad so iha somone meritatave\n",
      "========== Iteration  60  Loss:  40.3637615625  ==============\n",
      "inotrisoy ogi gt itat at id ig be bar!icag tat oni wit in in iny go\n",
      "\tms wiiniiy atse co cat if inares\n",
      "========== Iteration  61  Loss:  39.5618369004  ==============\n",
      "n tilithivit, valiantgitut hht vhigilith, in vavihighE linith ytrghendhey virginE, thingt ant win tin\n",
      "========== Iteration  62  Loss:  39.0368123286  ==============\n",
      "U\n",
      "\n",
      "Lind rine sittange Pt in thand st nt RAn, thandnnd: Tht this sotnit: wh: bes ir: vist ign :heeli t\n",
      "========== Iteration  63  Loss:  38.3001392166  ==============\n",
      "|NorB boue yirs ris rnEmr my mrr un frar pmouginenineumr: nitar:\n",
      "\n",
      "\n",
      "Hirs yS\ts fas my wur un wity fror \n",
      "========== Iteration  64  Loss:  37.5784256168  ==============\n",
      "C\tALingt\n",
      "\n",
      "PARhing mer\n",
      "\n",
      "PAROLVROLRRAlimy bew bs mPAROVS\tLES\tIn?\n",
      "\n",
      "PARef?\n",
      "PnROLsiRgingin toum und\n",
      "Ppo mP\n",
      "========== Iteration  65  Loss:  37.6210543488  ==============\n",
      ", igiS hip, wytinganither;\n",
      "\thar bhiowaw, bithincot hit, bdiwey, uy,\n",
      "\thow chin hy, bit, aner ane harw\n",
      "\n",
      "========== Iteration  66  Loss:  37.5586219008  ==============\n",
      "jPtar rat vavorer he\n",
      "\twreres wac it novun vesliter shentt ua uraner fadurt toruee toeret ure pre yoer\n",
      "========== Iteration  67  Loss:  36.8601541132  ==============\n",
      "urld gito\n",
      "\tyor virlirirghtito\n",
      "\tyilginirginity\n",
      "\tilg vigft\n",
      "\n",
      "\titr\n",
      "\tns\n",
      "\tilgt igt Pity\n",
      "\tithe yity nito\n",
      "\tin\n",
      "========== Iteration  68  Loss:  36.1077066113  ==============\n",
      "X]om noto vacgenVone be asc to.tbyonlcis fe tealy to g. inge co my bisy oncl once mastye yo vy to bac\n",
      "========== Iteration  69  Loss:  36.1010103846  ==============\n",
      ")\tLtLin;!\n",
      "\n",
      "HEwam awa' wovinithn\n",
      "\n",
      "\n",
      "HEy wy nilith\n",
      "\tin on wy wion. itp 'ol wiwiy L'myd'\n",
      "\tLEiry wad' on! \n",
      "========== Iteration  70  Loss:  35.3998676041  ==============\n",
      "CROhLOe R'd lith beelsce's a lgs birgin t APO' PLESbe cPAROacg sild bS\tTherc's ' I L LiES\tThere t meE\n",
      "========== Iteration  71  Loss:  34.9315655243  ==============\n",
      "-Hits of uicitg, ais cingt inc uisg, iy wocistisuco ackoustes co yf 'soucous of uo voure.,yous co,\n",
      "\tT\n",
      "========== Iteration  72  Loss:  34.8235522662  ==============\n",
      "RARit:\n",
      "\tvieg virginilivirgims\n",
      "\ts il:\n",
      "\tvirgin:\n",
      "\tvinginitharsit:\n",
      "\tiis\n",
      "\tvirginitv\n",
      "\tmirm\n",
      "\n",
      "\tAhins fird aw \n",
      "========== Iteration  73  Loss:  34.9997572191  ==============\n",
      "oup epf of ansee shans\n",
      "\tsomaetastanstofserseaperplamspeandios apas rpeasiepererenofere\n",
      "\toureerate\n",
      "\tes\n",
      "========== Iteration  74  Loss:  34.8031594718  ==============\n",
      "A\tisy nl pf be co thanc, an tp\tyung thL\tvarasemes its rt\n",
      "\tverst,\n",
      "\tvirg\n",
      "\tOuere fpe vareliny vacy virgh\n",
      "========== Iteration  75  Loss:  34.8003062349  ==============\n",
      "[Eofpoul, Thoudele of peldd owfidls,omdd-ede, of\n",
      "\tded,, ucg,,eeld ef isloun, ofe, ofe-elgesellese uve\n",
      "========== Iteration  76  Loss:  34.9386505192  ==============\n",
      ")\toacwy; os beoblot'n thotd ont obe\n",
      "\toanettey ost osouloye bot ouse; nttous belobey: my;\n",
      "\tsto woose\n",
      "\t\n",
      "========== Iteration  77  Loss:  34.5533861511  ==============\n",
      "jithich reipsers ise; anc thaca\n",
      "\tith; aire nhinainitre pincl;paicrimapeipideans inc yiirlird apinsinc\n",
      "========== Iteration  78  Loss:  34.2392035888  ==============\n",
      "ouc il to hlile miklithE so cA\n",
      "\n",
      "\tHon sere in tleAd to itse, sit gir, lo ile\n",
      "PEisit owg io wominit oig\n",
      "========== Iteration  79  Loss:  34.312806219  ==============\n",
      "Ztte my y ilimam: weis. woy mer m owl cirhille lom stodity ltklimy mike thl oucl waile my' moRlillima\n",
      "========== Iteration  80  Loss:  34.0429905876  ==============\n",
      "J its with;btheete the ithithyt bibdits; tere be vendith; lir the nesel tenferse be; cint;we; vinde''\n",
      "========== Iteration  81  Loss:  34.2109229011  ==============\n",
      "he\n",
      "\tliche\n",
      "\tudoulliitdtsin thirfuinuquluttiy cilit ittiwacains sis: cor: bt\n",
      "\tblouu:\n",
      "\tbf tue futhtility\n",
      "========== Iteration  82  Loss:  34.2600554081  ==============\n",
      ", ititorr Yuotorean toud dery rhirt ysir be to buitarh awoun der, . weey anr bour be dear betjink the\n",
      "========== Iteration  83  Loss:  33.4271026607  ==============\n",
      "ke virginity, is  iot pike orer, ose, onk, youlg, ar yi, ons in il inityies il kiigg our ing nour oln\n",
      "========== Iteration  84  Loss:  33.3658130794  ==============\n",
      "ll\n",
      "\n",
      "\twert; wicfert;\n",
      "\tat ehery, is\n",
      "\tmere at ntareee; pere; it wad var pe?, ry; pher ead rour ar aw yad\n",
      "========== Iteration  85  Loss:  32.831978696  ==============\n",
      "-nithly ity 'N\n",
      "\tNom'theNn with olt d tirg?y ond my, pitw youn ong mytofe\n",
      "\n",
      "\tLLENARANoLyoulg\n",
      "\tNornd it \n",
      "========== Iteration  86  Loss:  32.5300581413  ==============\n",
      "KRitterrear\n",
      "\tthererate\n",
      "\tTA\tof\n",
      "\tAf\n",
      "\t[ourssamamasresras r Atsersd,\n",
      "\tA mhis andes, mosts namstand,\n",
      "\tT\tAo\n",
      "========== Iteration  87  Loss:  32.3484455087  ==============\n",
      "FE calllersercorlar, a sorriar,,\n",
      "\tA yovenean ary, mathist,\tinity,\n",
      "\tArlernsedanleary,\n",
      "\tAr,destarsriltu\n",
      "========== Iteration  88  Loss:  32.3466595663  ==============\n",
      "\n",
      "\tHid dos, jad hicood and Hitcoud coundidis coss, dHord isd ord ladld,\n",
      "\tHis sord,\n",
      "\tHis res a drird, c\n",
      "========== Iteration  89  Loss:  32.5518384966  ==============\n",
      "XDitincandonsbtitirn,\n",
      "\tThpty, pinit orind, (isbrendptagld cumarheunomAnOyirl, Thit;\n",
      "\tThatb,\n",
      "\tTind mar\n",
      "========== Iteration  90  Loss:  32.7056077858  ==============\n",
      "S\tAR unce 'n a bep hein't. 'nd tha court'n hes wer 'ta shallne simla Oou'tn hli g: Gourl! sou - an ne\n",
      "========== Iteration  91  Loss:  32.2635748979  ==============\n",
      "'till-\n",
      "\n",
      "PAROLLES\tThity pA\tThat I -ill wes  PALit.--\n",
      "PAROhes' si't-\n",
      "\n",
      "NALALOTPAROOTE 'T-\n",
      "PNALis- -wi' s\n",
      "========== Iteration  92  Loss:  31.6547598057  ==============\n",
      "]\n",
      "\tThaat a towe helttpe; a be hight welg; that we, bet betheyot a the foll; felt pelt wellthet adl Th\n",
      "========== Iteration  93  Loss:  32.1329399372  ==============\n",
      "ll bufslowf tudom sowr welemf, onle mailoow dorr forlirfous uminout wom\n",
      "\n",
      "\n",
      "\tMulous focgEor forlg wor f\n",
      "========== Iteration  94  Loss:  32.3613888264  ==============\n",
      "?S\tMout ons oniser Pagevetaro arn\n",
      "\tMugterine]\n",
      "\n",
      "Pase]\n",
      "\tMagouraroukrer as rere]n\n",
      "\tMounorlouree ir Pager\n",
      "========== Iteration  95  Loss:  32.5799057678  ==============\n",
      "]\n",
      "\tIreew el I werembef.\t Limen, tfit, I ge be th I ch, wfmeumet\n",
      "\tOmicitererem,wRle be fweacen thie, c\n",
      "========== Iteration  96  Loss:  32.3651711812  ==============\n",
      "Thar.al aur a tharisab\n",
      "\n",
      "\n",
      "LANSE co rer t arlas\n",
      "\tMert babtab teder.\n",
      "\n",
      "\n",
      "PARELELAPMa sta, awe,\n",
      "\n",
      "\tLLthart a\n",
      "========== Iteration  97  Loss:  31.4722379817  ==============\n",
      "Jold yhy under Mars?\n",
      "\n",
      "HELEWEROLENA\tThandy LEps?\n",
      "\n",
      "LELES\tWhasd Mars?\n",
      "\n",
      "HELENA\tMu sh?\n",
      "MHr ch?\n",
      "\n",
      "PAROLELELE\n",
      "========== Iteration  98  Loss:  30.7026958032  ==============\n",
      "AROLLES\tWhen he thensseed he. ho sors he wade hor par phd mwas nome tpat ro than?\n",
      "\n",
      "LES\tWhen hos. of m\n",
      "========== Iteration  99  Loss:  30.0903910681  ==============\n",
      "Th \tYous Yo  NA\tYon ant?\n",
      "\n",
      "\n",
      "LENA\tYo so to Why think fo Yo Uashes yo mou go?\n",
      "\n",
      "ERLELENOE shinks, RA\tYo d\n",
      "========== Iteration  100  Loss:  30.2701892483  ==============\n",
      "d inang auringous chanl.S\tUane aun far hinken, Uoa,\n",
      "\tweithang, awevest racame uhin?\n",
      "\n",
      "HELENA\n",
      "ENA\t'o aw\n",
      "========== Iteration  101  Loss:  30.4690916955  ==============\n",
      "blit y u na thy or a a osigis an iane you is o inithe ir yo I sea inke fo io io kou iore anour arisou\n",
      "========== Iteration  102  Loss:  30.2690498451  ==============\n",
      "|S\twTlots wallMomitut acaresofelant. I cann jelleelt, I ce I fuminc, I Iut I yuteot.\n",
      "\n",
      "PAROLLES\tI cacw\n",
      "========== Iteration  103  Loss:  30.9824058482  ==============\n",
      "C\tSs, ll, thourt,\n",
      "\tA\twirla vaseeveltz\n",
      "\tThelot shenlt,\n",
      "\tWhat\n",
      "\tYeatoo vilat,\n",
      "\tin se no youtver ie Le th\n",
      "========== Iteration  104  Loss:  31.6241180418  ==============\n",
      "HEh ve fult thel thale wouc indest ur antthule heu shut allall; urll unde sh that\n",
      "\tthets\n",
      "\tbseest\n",
      "\tLE \n",
      "========== Iteration  105  Loss:  32.0918287656  ==============\n",
      "thel whia whour: wadl. thec; rather\n",
      "Pfear&.\n",
      "\tWhelel'\n",
      "\twh.\n",
      "\twiouratha:\n",
      "\tWlie thon\n",
      "\twhee fall\n",
      "\tWhon.\n",
      "\tW\n",
      "========== Iteration  106  Loss:  32.4776237537  ==============\n",
      "-\n",
      "\tbs end and and beed haghetuthage, boud hacug; gagugt lesdebe; be band bi ua; reld hes se bavelembe\n",
      "========== Iteration  107  Loss:  32.4517946653  ==============\n",
      " Hor tond hicir slrers;\n",
      "\tKu of, lomn fo, iodk thiesoud out lovedous ie lodsellies ureld fullf lo, inl\n",
      "========== Iteration  108  Loss:  33.221632253  ==============\n",
      "[E L or dwe sd build ulls wald wuckOhes ds urd ond oudcow diped bl: siskwarcor dat wald buscoprls pud\n",
      "========== Iteration  109  Loss:  33.28396037  ==============\n",
      "(\n",
      "\tOosv, mages mies,,\n",
      "\tWhat makese,\n",
      "\tThac, ase ghke, se ts hee; makmte saeum,\n",
      "\tTi moor in,, angimkike\n",
      "========== Iteration  110  Loss:  33.3450504627  ==============\n",
      "S\tTEojis in spinf la sis anliks in in soy join sik joat lis se jo n ks is nl is in ake sis se \t'n so \n",
      "========== Iteration  111  Loss:  33.2908833334  ==============\n",
      "rsis doon nge Opingo none so nas senns in ne sion po in thigh me sonde\n",
      "\tTha os ins in nies Gisd waisi\n",
      "========== Iteration  112  Loss:  33.4895610562  ==============\n",
      "PAROthem cors hid dirld:?hion sed d ves d he?\n",
      "\tAmivevivis hid mithes d him didiwhe rot hirs bt?\n",
      "\tI at\n",
      "========== Iteration  113  Loss:  33.6451149893  ==============\n",
      "che welle leind msilo fill ve x wyinn.\n",
      "\n",
      "\tA Ond not liveanvelvill se will ditvd wyd outs and fill fitv\n",
      "========== Iteration  114  Loss:  34.4503925765  ==============\n",
      "BENORtosel.\n",
      "\n",
      "\n",
      "\n",
      "SC\tI l.NThacas wal'c[,\n",
      "\tWo seseise.\n",
      "\n",
      "\t[Thace.\n",
      "HI ['y pol 'h, palice le.\n",
      "\n",
      "\tGoFls\n",
      "\t[ill \n",
      "========== Iteration  115  Loss:  34.6702072696  ==============\n",
      " With lendeseanleend Fine Giten and seln litee let ondeneannsentere FLoFloursertenvennhenit] FndeFlls\n",
      "========== Iteration  116  Loss:  35.098009729  ==============\n",
      "The wan.\n",
      "\n",
      "iis w o rins sirite  Fr t pors are f o d b\n",
      "\tth a th  irf v urd po Gitiwe sirg b n tiari gor\n",
      "========== Iteration  117  Loss:  35.531734656  ==============\n",
      "y,\tthatyithe itice weithirtive ve\ttiletricted ce htictiel ctereccive veithititheceivecewite d ceithit\n",
      "========== Iteration  118  Loss:  36.1287741295  ==============\n",
      "ROL 'turgiscoure wisirye Filive uFiuy way wheared;\n",
      "\tFlle wisharey\n",
      "\tFoore wied oreldyeoreusaeris;\n",
      "\tFin\n",
      "========== Iteration  119  Loss:  36.2879361062  ==============\n",
      "Theaslinld wikesU\t\n",
      "mLFaL\tThiLhtand\n",
      "\n",
      "HOLLL\n",
      "\n",
      "Lyoutiwl a vikl oremisheakd\n",
      "\n",
      "\tWhL\tThakl.\n",
      "\tThound\n",
      "\tThemind \n",
      "========== Iteration  120  Loss:  36.2334331637  ==============\n",
      "\n",
      "\n",
      "HaG.\n",
      "\tKppea ay mradecaness\n",
      "\tKpand ne.\n",
      "\n",
      "Hppred cesae.\n",
      "\n",
      "HprN\tKlaned\n",
      "\n",
      "KING\tHN\tHrom or lle cert aract\n",
      "\n",
      "\n",
      "========== Iteration  121  Loss:  36.2952148041  ==============\n",
      "-Haa te meenceme mee tan tht des lgane a ] an heme aen leme mete meem ale toor gere teo Fltmee Lemen \n",
      "========== Iteration  122  Loss:  36.5623727301  ==============\n",
      "tiriles mad Lerd\tIs ond Gord\tIm\n",
      "\tAy corr wI\tIatt\n",
      "\n",
      "menceer Save\n",
      "\tmoy se le lemes\n",
      "\n",
      "\n",
      "GA seat tor\tls\n",
      "\tImy\n",
      "========== Iteration  123  Loss:  36.8430961246  ==============\n",
      "d hor se\n",
      "\t'k le reses se\n",
      "\tFen'del here ar,\n",
      "\tWhat be be ces?\n",
      "\t\tWopt thore's?\n",
      "\tWheat er'sen he ce comic\n",
      "========== Iteration  124  Loss:  36.9704639228  ==============\n",
      "qurirls ford,\n",
      "\tYot,\n",
      "\tYoun Blo,\n",
      "\tYorord meresold,\n",
      "\tYougind loud lhomdy, Rird ind ond mhend merd os BYo\n",
      "========== Iteration  125  Loss:  36.2767445716  ==============\n",
      "xite, nather surs, nathe,\n",
      "\tacure; thr Frane ancur bourer arur, fatherr antheave uras father coure,\n",
      "\tU\n",
      "========== Iteration  126  Loss:  36.2740572586  ==============\n",
      "I\n",
      "\tOat L tho. thll loo! Mort yel'tin too!\tWoinile ith  ! mio! Pouelitit hor!emilcot Uilit!\n",
      "\tHan t o M\n",
      "========== Iteration  127  Loss:  36.0648648336  ==============\n",
      "]\n",
      "\n",
      "\n",
      "\tInpar,\n",
      "\n",
      "KING\tIc and nosst\n",
      "\tKrpalast bnd tsor comas.\n",
      "\tI waras wadcos thand I prend torsssere sor \n",
      "========== Iteration  128  Loss:  36.108320336  ==============\n",
      "inds and thidk lioud lo he kef lo sis! ar\n",
      "\n",
      "\tI keie! oud thekdier hidoth\n",
      "\tFI diind hir! Ph dhifpoerali\n",
      "========== Iteration  129  Loss:  36.103519398  ==============\n",
      "D Trot do gid hisg bgou ggthd ing dod had thad both hang d loth ulos co lisd gisel hilc hath un d boo\n",
      "========== Iteration  130  Loss:  36.495399174  ==============\n",
      "S\tOst yomrs of brol. Indfor t or toot fay\n",
      "\tTod I taod heryfarg.\n",
      "\tB goou. of ars.\n",
      "\tI lo at in sig.\n",
      "\tI \n",
      "========== Iteration  131  Loss:  36.0712496111  ==============\n",
      ". Thille vank yord\tInd mag thay mar brsy rat they map it thert yor sel mortin mat jay je\n",
      "\tTont; the b\n",
      "========== Iteration  132  Loss:  36.173708995  ==============\n",
      "The core linveir in our oco oun theil louthic lo bour Somitroun lor'arurey eik ol we ig; youl ink the\n",
      "========== Iteration  133  Loss:  36.2146944637  ==============\n",
      "ros and has al and them aak shand thead has aqued, laad pad they; hay jad eq aas ab walits\n",
      "\tH pord\n",
      "\n",
      "H\n",
      "========== Iteration  134  Loss:  36.2572348812  ==============\n",
      "hem,\n",
      "\tEE bamow him cies mad hidis ind mar, abn take hid hikes ankeC\tSice d\n",
      "\tHus lake  fep hisis dimak\n",
      "========== Iteration  135  Loss:  36.1829411927  ==============\n",
      "BEx athow and gere weacne\n",
      "\tpe gerof uw'\n",
      "\tInd berce\n",
      "\ttha s\n",
      "\tAnd bod fnpof matus of ar obwish lf I and\n",
      "\n",
      "========== Iteration  136  Loss:  35.8475484766  ==============\n",
      "G\tTr of treis po me hep lors murin huprom rur hure, himip riler pora) their riy he hs praiser humats \n",
      "========== Iteration  137  Loss:  36.0101280392  ==============\n",
      ";\n",
      "\t del; wemoustsenthend himusor tow wom treed wo ll wlagd of tunc, woudstsend towld, would he of the\n",
      "========== Iteration  138  Loss:  35.9486584923  ==============\n",
      "]\n",
      "\tLithey ther ur\n",
      "\tEn them in hichgon um rink your themR't cour thoun tyou, then, thoughas thor\n",
      "\tAh r\n",
      "========== Iteration  139  Loss:  36.3649159078  ==============\n",
      "U\tWit pow thell wis sered whe wher per would I\n",
      "\tWit woud towGKKING\tWher thald theld lill wis oprincI \n",
      "========== Iteration  140  Loss:  36.8068918142  ==============\n",
      "Mirisd'tand unter iot un t ant'sterterter'sts Het tos ericend totrd and.\n",
      "\tHe bar it cotrend In int ep\n",
      "========== Iteration  141  Loss:  36.5548961453  ==============\n",
      "C\tTit food, thergheman, bis good,\n",
      "\tThid loong min sow blan hill, bot moond them,\n",
      "\n",
      "Tom loor bIomn ow b\n",
      "========== Iteration  142  Loss:  36.5215474937  ==============\n",
      "ll turb,e,''a'tan' 't not ther, qhis  Lou his out, he,\n",
      "\t' the co,\n",
      "\t'Fet,,\n",
      "\t'stimus,--' lot loow them-\n",
      "========== Iteration  143  Loss:  37.2026165492  ==============\n",
      "& sessnves sellesingy np levels nehris altet gelds\n",
      "\tAneneivel nelten senkhr, nensea;swersirs thises\n",
      "\t\n",
      "========== Iteration  144  Loss:  37.6070274098  ==============\n",
      "quE pedin wofits; bericuthingeieager\n",
      "\tExceerale bers antsie ae Cirtrowemyofthepenineweleareistimirs\n",
      "\t\n",
      "========== Iteration  145  Loss:  37.4632502894  ==============\n",
      "is or wandsr'  nor I,\n",
      "\tSher waxd our So xot I I whe bonen; an ofr\n",
      "\n",
      "\tI rox hin somin,\n",
      "\tS wox an of iar\n",
      "========== Iteration  146  Loss:  37.5443258839  ==============\n",
      "-\n",
      "\tof our jomed demod deroy iong ood.\n",
      "\tSovod malo roand. lov.y Yobser somem\n",
      "\tYelous.r cad.\n",
      "\tYore Yoor\n",
      "========== Iteration  147  Loss:  37.160546023  ==============\n",
      "of back can. lo K p. I wis toll lo llpll I milo anck ala loaftencI\n",
      "\tINN't whill, lood ou l cas lakd I\n",
      "========== Iteration  148  Loss:  37.1415592486  ==============\n",
      "ay f d fad\n",
      "\n",
      "SA   RTRAM I d d mfar fupo, d    d   che.\n",
      "\tB Fit eud my \n",
      "\n",
      "BTREMABERTRA S\tThad mas mas aud\n",
      "========== Iteration  149  Loss:  36.733035658  ==============\n",
      "-'Tand tend., vathe,\n",
      "\tKend me lamene ther the nce theldtim, lo rem det, I y the mivin. mumem. the bit\n",
      "========== Iteration  150  Loss:  36.9447566266  ==============\n",
      "ROLLe and in's ioutait aadeittees weyiteiser dts alinemate dDelisdimestes becatn tetibe teps;;\n",
      "\tDeset\n",
      "========== Iteration  151  Loss:  36.6396799558  ==============\n",
      "Flo thay Ehje]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\tly [hunk\n",
      "\n",
      "R[ELI\n",
      "\n",
      "\t[EF bour;\n",
      "\n",
      "mELpyourulyN\n",
      "\n",
      "\n",
      "'A] sunt.\n",
      "\n",
      "B[ he oure.\n",
      "\n",
      "\n",
      "NARAnkithink\n",
      "========== Iteration  152  Loss:  36.969424674  ==============\n",
      "|E'U\t[Enos CI lace, jecat laceder shet anet.\n",
      "CENUNS\t[ENTES [ExCener: ansert mes lyce yS Ceckerpter Ca\n",
      "========== Iteration  153  Loss:  37.1139654466  ==============\n",
      ": ater geid wen melas?\n",
      "\tSowand the ald gererad?\n",
      "\tAS\tI thld Mhowalamast,\n",
      "\n",
      "SM\tHit gelowaromand werat we\n",
      "========== Iteration  154  Loss:  36.9504729308  ==============\n",
      "ked mas!\n",
      "\t[I fave madanst mous\n",
      "\tMut nounos fa tonos oant; wo mpsmy of myvers; pade\n",
      "\tOund; fast fay,\n",
      "\n",
      "\n",
      "========== Iteration  155  Loss:  36.5513685783  ==============\n",
      "jE bldyess woounovesvs fe;\n",
      "\n",
      "\n",
      "\tdis nfunoveslcas vore of our af sunNves, ounsen mis ofreils\n",
      "\tuns desern\n",
      "========== Iteration  156  Loss:  36.6935498995  ==============\n",
      "plower doa has: Ibe malas mavaves coed this mas loel,\n",
      "\tGelamave Ithithe has hem: majlathithe Calave a\n",
      "========== Iteration  157  Loss:  36.4705279553  ==============\n",
      "jitind coonplo: litl that fy' cor yoin toon hft;\n",
      "\tGitl the cot\n",
      "\tfit oy my kots colk hor y loo coonl: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8b60723caa1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mhallucinate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mRunModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-8b60723caa1a>\u001b[0m in \u001b[0;36mRunModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mh_state\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_out\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/juser/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/juser/.local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 428\u001b[1;33m                                target_list)\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def RunModel():\n",
    "    global ix,losses,iterations,h_state,smooth_loss\n",
    "    run = 0\n",
    "    while True:\n",
    "        letters=[]\n",
    "        if ix + num_steps >= len(data):\n",
    "            break\n",
    "        input_chars  = [make_hot_vec([1,vocab_size], char_to_ix[ch]) for ch in data[ix:ix+num_steps]]\n",
    "        target_chars = [make_hot_vec([1,vocab_size], char_to_ix[ch]) for ch in data[ix+1:ix+1+num_steps]]\n",
    "\n",
    "        feed_dict={inputs[i]: input_chars[i] for i in range(len(input_chars))}\n",
    "        feed_dict.update({truths[i]: target_chars[i] for i in range(len(target_chars))})\n",
    "        feed_dict.update({hs[-1]:h_state})\n",
    "\n",
    "        _, h_state, loss_out = sess.run((train_step, hs[num_steps-1], cross_entropy), feed_dict=feed_dict)\n",
    "\n",
    "        smooth_loss = smooth_loss * 0.999 + loss_out * 0.001\n",
    "        ix += 1\n",
    "        #        letters.append(data[ix])\n",
    "        run += 1\n",
    "        if run % 100 == 0:   \n",
    "            iterations += 1\n",
    "            #        print \"train: \", ''.join(letters)\n",
    "            print \"========== Iteration \", iterations, \" Loss: \", smooth_loss, \" ==============\"\n",
    "            losses.append(smooth_loss)\n",
    "            hallucinate(sess, random.randint(0,vocab_size-1))\n",
    "            \n",
    "RunModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
