{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data!\n",
    "------------\n",
    "Shakespeare's comedies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 2108622 characters, 69 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('all_comedies_cat.txt', 'r').read() # should be simple plain text file\n",
    "#data = \"abcdefghijkabcdefghijk\"\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_hot_vec(size, t):\n",
    "    ''' size: a list of dimensions, like is passed to np functions.\n",
    "        t:    the index to set to 1 (the truth).\n",
    "    '''\n",
    "    xs = np.zeros(size) # encode in 1-of-k representation\n",
    "    xs[0,t] = 1\n",
    "    return xs\n",
    "make_hot_vec([1,4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters -- These control the model's behavior. It's nice to group them so you can\n",
    "# change them together.\n",
    "hidden_size = 100\n",
    "num_steps = 25\n",
    "learning_rate = 0.01\n",
    "\n",
    "# This is just for printing our progress.\n",
    "num_runs_between_logging = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----------\n",
    "Define the Model.\n",
    "------\n",
    "This is modelled after Karpathy's minimal RNN, described in his [excellent article](karpathy.github.io/2015/05/21/rnn-effectiveness/) and implemented in this [short gist](https://gist.github.com/karpathy/d4dee566867f8291f086). The basic step function is:\n",
    "\n",
    "```\n",
    "# update the hidden state\n",
    "self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "# compute the output vector\n",
    "y = np.dot(self.W_hy, self.h)\n",
    "```\n",
    "\n",
    "There are also two biases, `bh` and `by`. So the model has 5 learned parameters: `Wxh`, `Whh`, `Why`, `bh`, and `by`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a single step in the RNN from one char to the next\n",
    "Wxh = tf.Variable(tf.random_uniform([vocab_size, hidden_size], maxval=0.01)) \n",
    "Whh = tf.Variable(tf.random_uniform([hidden_size, hidden_size], maxval=0.01))\n",
    "bh = tf.Variable(tf.zeros([hidden_size]))\n",
    "Why = tf.Variable(tf.random_uniform([hidden_size, vocab_size], maxval=0.01)) \n",
    "by = tf.Variable(tf.zeros([vocab_size])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###Set up the recurrence\n",
    "\n",
    "This is important: A recurrent neural net is *recurrent* because the simple model is repeated multiple times to create the overall model. This is called \"unrolling\" the net.\n",
    "\n",
    "This matters, because in order for backpropogation to calculate the effect that the hidden state has on the loss, it needs to take the changes from one `state` to another into account when calculating the gradient. If you didn't unroll the network, and instead just returned the `state` from each run and passed the `state` back into the next run, the `state`'s only effect on the cost would be how the `state` effected `y`, not how the current `state` effected the next state.\n",
    "\n",
    "For example, the simple network without unrolling it looks like this:\n",
    "  \n",
    "$  cost = truth - y $, where $ y = W_{hy}*h' $ and $h' = tanh(W_{hh}*h + W_{xh}*x) $\n",
    "\n",
    "The current state, $h$, only effects $cost$ through it's impact on $y$. Even though it sets the *next state*, $h'$, the transition from $h$ to $h'$ is never considered during backpropogation.\n",
    "\n",
    "Instead, an *unrolled* network *does* effect the cost both from the current $h$ and on its effect on the next $h$:\n",
    "\n",
    "$  cost = (truth_1 - y_1) + (truth_0 - y_0) $\n",
    "\n",
    "for $ y_1 = W_{hy}*h_1 $, $h_1 = tanh(W_{hh}*h_0 + W_{xh}*x_1) $\n",
    "\n",
    "and\n",
    "$ y_0 = W_{hy}*h_0 $, $h_0 = tanh(W_{hh}*h + W_{xh}*x_0) $\n",
    "\n",
    "Now, $W_{hh}$'s impact on the next $h$ effects the cost just as much as its impact on the current $y$.\n",
    "\n",
    "SO, I *think*, the more steps you unroll a network for, the more emphasis you're placing on the hidden state's impact versus the other weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=TensorShape([Dimension(None), Dimension(69)]), dtype=float32) Tensor(\"Placeholder_1:0\", shape=TensorShape([Dimension(None), Dimension(69)]), dtype=float32) Tensor(\"Placeholder_25:0\", shape=TensorShape([Dimension(None), Dimension(100)]), dtype=float32) Tensor(\"Tanh:0\", shape=TensorShape([Dimension(None), Dimension(100)]), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inputs = [tf.placeholder(tf.float32, [None, vocab_size]) for _ in xrange(num_steps)]\n",
    "outputs = {}\n",
    "hs = {}\n",
    "hs[-1] = tf.placeholder(tf.float32, [None, hidden_size])\n",
    "for i in range(len(inputs)):\n",
    "    hs[i]      = tf.nn.tanh(tf.matmul(inputs[i], Wxh) + tf.matmul(hs[i-1], Whh) + bh)\n",
    "    outputs[i] = tf.nn.softmax(tf.matmul(hs[i], Why) + by)\n",
    "print inputs[0], inputs[1], hs[-1], hs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# Variables must be initialized by running an `init` Op after having\n",
    "# launched the graph.  We first have to add the `init` Op to the graph.\n",
    "init_op = tf.initialize_all_variables()\n",
    "# Run the 'init' op\n",
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----\n",
    "Training\n",
    "-------\n",
    "\n",
    "This is the \"cross-entropy\" algorithm.\n",
    "$$\n",
    "H_{y'}(y)= âˆ’\\sum_i y_i' \\log(y_i)\n",
    "$$\n",
    " $-\\log(y_i)$ is useful because it turns percentages into (0,$\\infty$) \"cost\".\n",
    "So then I think multiplying by the truth cancels out the percentages for everything else, and summing just turns the one_hot vector into a value?\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement Cross Entropy:\n",
    "truths = [tf.placeholder(tf.float32, [None, vocab_size]) for _ in xrange(num_steps)]\n",
    "cross_entropies = [-tf.reduce_sum(truths[i]*tf.log(outputs[i])) for i in xrange(num_steps)]  # These operations act element-wise.\n",
    "cross_entropy = tf.add_n(cross_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This was big. I changed the optimizer to the AdagradOptimizer, and upped the learning\n",
    "#  rate from 0.01 to 0.1.\n",
    "# I think it's okay to increase the learning rate because Adagrad decreases the rate over time.\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Do the thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##Hallucinating! The fun part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hallucinate(sess, seed_ix, num_chars):\n",
    "    ix = seed_ix\n",
    "    ixes = [ix]\n",
    "    hallucination_h_state = np.zeros([1, hidden_size]) \n",
    "    for i in range(num_chars):\n",
    "        # Create a random starting letter \n",
    "        x_in = make_hot_vec([1,vocab_size], ix)\n",
    "\n",
    "        feed_dict={inputs[0]: x_in}\n",
    "        feed_dict.update({hs[-1]:hallucination_h_state})\n",
    "\n",
    "        output, hallucination_h_state = sess.run((outputs[0],hs[0]), feed_dict=feed_dict)\n",
    "        probs = output[0]\n",
    "        ix = np.random.choice(range(len(probs)), p=probs)\n",
    "        ixes.append(ix)\n",
    "    hallucination=''.join([ix_to_char[ix] for ix in ixes])\n",
    "    print hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "iterations = 0\n",
    "ix = 0\n",
    "h_state = np.zeros([1, hidden_size])\n",
    "smooth_loss = -np.log(1.0/vocab_size)*num_steps # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9bda0264dbd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#RunModel()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mOut\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 16"
     ]
    }
   ],
   "source": [
    "def RunModel():\n",
    "    global ix,losses,iterations,h_state,smooth_loss\n",
    "    run = 0\n",
    "    while True:\n",
    "        letters=[]\n",
    "        if ix + num_steps >= len(data):\n",
    "            break\n",
    "        input_chars  = [make_hot_vec([1,vocab_size], char_to_ix[ch]) for ch in data[ix:ix+num_steps]]\n",
    "        target_chars = [make_hot_vec([1,vocab_size], char_to_ix[ch]) for ch in data[ix+1:ix+1+num_steps]]\n",
    "\n",
    "        feed_dict={inputs[i]: input_chars[i] for i in range(len(input_chars))}\n",
    "        feed_dict.update({truths[i]: target_chars[i] for i in range(len(target_chars))})\n",
    "        feed_dict.update({hs[-1]:h_state})\n",
    "\n",
    "        _, h_state, loss_out = sess.run((train_step, hs[num_steps-1], cross_entropy), feed_dict=feed_dict)\n",
    "\n",
    "        smooth_loss = smooth_loss * 0.999 + loss_out * 0.001\n",
    "        # Iterate by 1 or num_steps? I guess by 1 could lead to overfitting?\n",
    "        ix += num_steps\n",
    "        #        letters.append(data[ix])\n",
    "        run += 1\n",
    "        if run % num_runs_between_logging == 0:   \n",
    "            iterations += 1\n",
    "            #        print \"train: \", ''.join(letters)\n",
    "            print \"========== Iteration \", iterations, \" Loss: \", smooth_loss, \" chars \", str(ix)+\"/\"+str(len(data)), \" ==============\"\n",
    "            losses.append(smooth_loss)\n",
    "            hallucinate(sess, random.randint(0,vocab_size-1), 100)\n",
    "            \n",
    "RunModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "npanc, soelce we rettipgb thengt sasd.\n",
      "\tTorpaTp and s at olk, oft wrenlthann' thebasf ahe afts bocikd ard,\n",
      "\tThe krorel warth urp ankoat noud, tha?\n",
      "\tPaom feess \n",
      ", at an fow ivin\n",
      "\tFt thind: nop this finis\n",
      "\tTo ce nvin]\n",
      "\tse nou.\n",
      "\tOnd yo ind.\n",
      "SPRPUGVEUA\n",
      "\tAy,\n",
      "\tHot and movalG I mathe yoe the surd ly bob  oms fhag;\n",
      "\n",
      "jNUUOGI I: Ny\n",
      "\tWim Mic, sot Aiat an foandend!\n",
      "\tBone af\n",
      "\n",
      "JEPxOMUOS\n",
      " Iamer\tIg ar; were ot enwanpsours miy fot ians paleas ank\n",
      "\n",
      "AEBSIUS\n",
      "PLRUGUIN\tSoeessp!ohs, witcels neo mithe sfol andor.\n",
      "\tAt meacin- opd the merbbmat ny sat' mar vas  he thiil mam: we ser,\n",
      "\tCir: uhe meotin' hin sramndaenbes mf a tuy bpen fneefthisnjorertar lord yosb eou thy to secode:.\n",
      "\tThitr soy erespakk Yocgteebind a lace dimre of wires\n",
      "\n",
      "IM\tI che thar wanss\tdertin. oo lf turt wacice thue bond aive the beint ou dsecherd\n",
      "\tThint,;\n",
      "\tBo as cilg or yon hic |atroun melse\n",
      "\tthes wrothiu s\n",
      "\t[om wibed\n",
      "\tnou rofetont sinde\n",
      "\n",
      "\tAmelr haylpe merir iol\n",
      "\tArekP\tyor cerethan, srorinvod the the toretk nunos I terce\n",
      "\tPhelarine.\n",
      "\n",
      "AFREPCOEGEE\n"
     ]
    }
   ],
   "source": [
    "hallucinate(sess, random.randint(0,vocab_size-1), 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RunModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
