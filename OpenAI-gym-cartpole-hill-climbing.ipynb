{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Climbing Algorithm for CartPole OpenAI Gym\n",
    "This file implements the simplest Reinforcement Learning solution to the intro-to-RL CartPole problem from the OpenAI Gym (https://openai.com/requests-for-research/#cartpole), the \"*hill-climbing algorithm*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-12 00:59:45,964] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highs: [  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "Lows:  [ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(\"Highs:\", env.observation_space.high)\n",
    "print(\"Lows: \", env.observation_space.low)\n",
    "\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03775622,  0.04316844,  0.00642069, -0.04089588])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I'm just keeping track of my personal best. This has to be updated manually.\n",
    "# ... I happened to randomly get a perfect model in the first 100 I tried (number 63)!\n",
    "# They seem to happen in about 1/150 of the models I generate.\n",
    "personal_best_reward = 1000\n",
    "personal_best_weight = np.array([ 0.10047517,  0.45675998,  0.99510988,  0.75130867])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HyperParameters:\n",
    "learning_rate = 1  # There is no reason to keep this small, since we're perturbing by a random amount within this range.\n",
    "                   # Since this isn't gradient descent and we only keep ones that perform better, you don't have the\n",
    "                   # \"overshooting\" problem of too large a learning_rate. Here I've set it to the whole space.\n",
    "num_runs = 10000\n",
    "num_batches = 100\n",
    "max_num_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_range(a,b,shape):\n",
    "    return (b-a) * np.random.random(shape) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.18743879, -0.22296214,  0.24887228,  0.36269794])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_initial = random_range(-1,1,env.observation_space.shape)\n",
    "W_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_step(W,x):\n",
    "    ''' Simplest model ever: Just the linear multiplication, i.e. dot product!\n",
    "    Technically this is a logistic regression, which chooses between two classes. '''\n",
    "    y = np.dot(W,x)\n",
    "    return [0,1][y >= 0] # Use sign of result to decide left or right.\n",
    "model_step(W_initial,observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Replacing old model! 12 better than 10 --\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.18743879, -0.22296214,  0.24887228,  0.36269794]),\n",
       " array([ 0.43936486,  0.94313114, -0.60929714,  0.14942928]),\n",
       " array([ 0.95948229,  1.38377709, -1.36906444,  0.89878144]),\n",
       " array([ 0.43936486,  0.94313114, -0.60929714,  0.14942928]),\n",
       " 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_model(W_prev,score_prev, W, score):\n",
    "    ''' Randomly perturb the weights, and if it performs better than last time, keep it. '''\n",
    "    keep_W = W_prev\n",
    "    keep_score = score_prev\n",
    "    if score > score_prev:\n",
    "        keep_W = W\n",
    "        keep_score = score\n",
    "        print(\"-- Replacing old model! {0} better than {1} --\".format(score, score_prev))\n",
    "    new_W = np.copy(keep_W)\n",
    "    new_W += random_range(-learning_rate, learning_rate, new_W.shape)\n",
    "    return new_W, keep_W, keep_score\n",
    "Wb = random_range(-1,1,env.observation_space.shape)\n",
    "new_W, prev_W, prev_score = update_model(W_initial,10, Wb, 12)\n",
    "W_initial, Wb, new_W, prev_W, prev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_possible_reward():\n",
    "    return max_num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18595.0, 185.95)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model(W):\n",
    "    total_reward = 0\n",
    "    for i_episode in range(num_batches):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        batch_reward = 0\n",
    "        for _ in range(max_num_steps):\n",
    "            #env.render()\n",
    "            action = model_step(W,observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            batch_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        #print(\"Batch Reward: {}\".format(batch_reward))\n",
    "        total_reward += batch_reward\n",
    "    average_reward = total_reward/num_batches\n",
    "    return total_reward,average_reward\n",
    "\n",
    "test_model(W_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prev_weights = W_initial\n",
    "prev_reward = 0\n",
    "W = np.copy(prev_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/10000: Average Reward: 218.75 Total Reward: 21875.0\n",
      "-- Replacing old model! 21875.0 better than 0 --\n",
      "1/10000: Average Reward: 32.57 Total Reward: 3257.0\n",
      "2/10000: Average Reward: 9.94 Total Reward: 994.0\n",
      "3/10000: Average Reward: 9.12 Total Reward: 912.0\n",
      "4/10000: Average Reward: 475.67 Total Reward: 47567.0\n",
      "-- Replacing old model! 47567.0 better than 21875.0 --\n",
      "5/10000: Average Reward: 64.47 Total Reward: 6447.0\n",
      "6/10000: Average Reward: 279.74 Total Reward: 27974.0\n",
      "7/10000: Average Reward: 9.84 Total Reward: 984.0\n",
      "8/10000: Average Reward: 32.27 Total Reward: 3227.0\n",
      "9/10000: Average Reward: 20.13 Total Reward: 2013.0\n",
      "10/10000: Average Reward: 338.02 Total Reward: 33802.0\n",
      "11/10000: Average Reward: 55.66 Total Reward: 5566.0\n",
      "12/10000: Average Reward: 42.19 Total Reward: 4219.0\n",
      "13/10000: Average Reward: 58.15 Total Reward: 5815.0\n",
      "14/10000: Average Reward: 323.63 Total Reward: 32363.0\n",
      "15/10000: Average Reward: 463.32 Total Reward: 46332.0\n",
      "16/10000: Average Reward: 38.31 Total Reward: 3831.0\n",
      "17/10000: Average Reward: 107.91 Total Reward: 10791.0\n",
      "18/10000: Average Reward: 588.31 Total Reward: 58831.0\n",
      "-- Replacing old model! 58831.0 better than 47567.0 --\n",
      "19/10000: Average Reward: 708.56 Total Reward: 70856.0\n",
      "-- Replacing old model! 70856.0 better than 58831.0 --\n",
      "20/10000: Average Reward: 1000.0 Total Reward: 100000.0\n",
      "-- Replacing old model! 100000.0 better than 70856.0 --\n",
      "Best Reward: 100000.0\n",
      "Best Weight: [ 0.11671831  0.1011577   2.2157191   0.29025011]\n",
      "It's a NEW LAP RECORD!: 100000.0\n",
      "[ 0.11671831  0.1011577   2.2157191   0.29025011]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(num_runs):\n",
    "    global prev_weights,prev_reward,W\n",
    "    total_reward,average_reward = test_model(W)\n",
    "    print(\"{0}/{1}: Average Reward: {2} Total Reward: {3}\".format(idx, num_runs, average_reward, total_reward))\n",
    "    W, prev_weights, prev_reward = update_model(prev_weights, prev_reward, W, total_reward)\n",
    "    \n",
    "    if average_reward == max_possible_reward():\n",
    "        break\n",
    "    \n",
    "best_weights,best_weight_reward = W,total_reward\n",
    "print(\"Best Reward:\", best_weight_reward)\n",
    "print(\"Best Weight:\", best_weights)\n",
    "\n",
    "if best_weight_reward > personal_best_reward:\n",
    "    print(\"It's a NEW LAP RECORD!: {0}\".format(best_weight_reward))\n",
    "    print(best_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11671831,  0.1011577 ,  2.2157191 ,  0.29025011])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def render_model(W):\n",
    "    total_reward = 0\n",
    "    num_batches = 5\n",
    "    for i_episode in range(num_batches):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        batch_reward = 0\n",
    "        print(\"{0}/{1}:\".format(i_episode, num_batches))\n",
    "        for _ in range(5000):\n",
    "            #env.render()  # I don't think you can get this to render from MyBinder. :(\n",
    "            action = model_step(W,observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            batch_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(\"{0}/{1}: Batch Reward: {2}\".format(i_episode, num_batches, batch_reward))\n",
    "        total_reward += batch_reward\n",
    "    average_reward = total_reward/num_batches\n",
    "    return total_reward,average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5:\n",
      "0/5: Batch Reward: 5000.0\n",
      "1/5:\n",
      "1/5: Batch Reward: 5000.0\n",
      "2/5:\n",
      "2/5: Batch Reward: 5000.0\n",
      "3/5:\n",
      "3/5: Batch Reward: 5000.0\n",
      "4/5:\n",
      "4/5: Batch Reward: 5000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000.0, 5000.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5:\n",
      "0/5: Batch Reward: 5000.0\n",
      "1/5:\n",
      "1/5: Batch Reward: 5000.0\n",
      "2/5:\n",
      "2/5: Batch Reward: 5000.0\n",
      "3/5:\n",
      "3/5: Batch Reward: 5000.0\n",
      "4/5:\n",
      "4/5: Batch Reward: 5000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000.0, 5000.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(personal_best_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
