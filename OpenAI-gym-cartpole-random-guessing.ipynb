{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Guessing Algorithm for CartPole OpenAI Gym\n",
    "This file implements the simplest Reinforcement Learning solution to the intro-to-RL CartPole problem from the OpenAI Gym (https://openai.com/requests-for-research/#cartpole), the \"*random guessing algorithm*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-11 06:19:34,660] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highs: [  4.80000000e+00   3.40282347e+38   4.18879020e-01   3.40282347e+38]\n",
      "Lows:  [ -4.80000000e+00  -3.40282347e+38  -4.18879020e-01  -3.40282347e+38]\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(\"Highs:\", env.observation_space.high)\n",
    "print(\"Lows: \", env.observation_space.low)\n",
    "\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04240727, -0.00041078,  0.0115244 ,  0.02211273])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here I'm just keeping track of my personal best. This has to be updated manually.\n",
    "# ... I happened to randomly get a perfect model in the first 100 I tried (number 63)!\n",
    "# They seem to happen in about 1/150 of the models I generate.\n",
    "personal_best_reward = 1000\n",
    "personal_best_weight = np.array([ 0.10047517,  0.45675998,  0.99510988,  0.75130867])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_range(a,b,shape):\n",
    "    return (b-a) * np.random.random(shape) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.5325113 ,  0.87059501, -0.61881113,  0.91631689]),\n",
       " array([-0.59392468, -0.90155906, -0.83717758, -0.25264425]),\n",
       " array([-0.88522628, -0.41954885,  0.42927628,  0.38708858]),\n",
       " array([ 0.37939376, -0.58231667,  0.36979234,  0.58605575]),\n",
       " array([-0.15183026, -0.20212978, -0.75272117, -0.86942718])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bunch of random weights vectors, of the same shape as the input.\n",
    "# That way, you can dot-product the input and the weight to make a choice.\n",
    "Ws = [random_range(-1,1,env.observation_space.shape) for _ in range(100)]\n",
    "Ws[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_step(W,x):\n",
    "    ''' Simplest model ever: Just the linear multiplication, i.e. dot product! '''\n",
    "    y = np.dot(W,x)\n",
    "    return [0,1][y >= 0] # Use sign of result to decide left or right.\n",
    "model_step(Ws[0],observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8201.0, 82.01)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model(W):\n",
    "    total_reward = 0\n",
    "    num_batches = 100\n",
    "    for i_episode in range(num_batches):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        batch_reward = 0\n",
    "        for _ in range(1000):\n",
    "            #env.render()\n",
    "            action = model_step(W,observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            batch_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        #print(\"Batch Reward: {}\".format(batch_reward))\n",
    "        total_reward += batch_reward\n",
    "    average_reward = total_reward/num_batches\n",
    "    return total_reward,average_reward\n",
    "\n",
    "test_model(Ws[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_weights = None\n",
    "best_weights_idx = 0\n",
    "best_weight_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/100: Average Reward: 49.61 Total Reward: 4961.0\n",
      "1/100: Average Reward: 25.28 Total Reward: 2528.0\n",
      "2/100: Average Reward: 86.94 Total Reward: 8694.0\n",
      "3/100: Average Reward: 89.66 Total Reward: 8966.0\n",
      "4/100: Average Reward: 8.9 Total Reward: 890.0\n",
      "5/100: Average Reward: 108.13 Total Reward: 10813.0\n",
      "6/100: Average Reward: 139.21 Total Reward: 13921.0\n",
      "7/100: Average Reward: 39.98 Total Reward: 3998.0\n",
      "8/100: Average Reward: 9.17 Total Reward: 917.0\n",
      "9/100: Average Reward: 9.69 Total Reward: 969.0\n",
      "10/100: Average Reward: 40.86 Total Reward: 4086.0\n",
      "11/100: Average Reward: 52.21 Total Reward: 5221.0\n",
      "12/100: Average Reward: 9.08 Total Reward: 908.0\n",
      "13/100: Average Reward: 60.44 Total Reward: 6044.0\n",
      "14/100: Average Reward: 25.94 Total Reward: 2594.0\n",
      "15/100: Average Reward: 9.55 Total Reward: 955.0\n",
      "16/100: Average Reward: 9.08 Total Reward: 908.0\n",
      "17/100: Average Reward: 44.31 Total Reward: 4431.0\n",
      "18/100: Average Reward: 9.27 Total Reward: 927.0\n",
      "19/100: Average Reward: 32.85 Total Reward: 3285.0\n",
      "20/100: Average Reward: 9.63 Total Reward: 963.0\n",
      "21/100: Average Reward: 50.82 Total Reward: 5082.0\n",
      "22/100: Average Reward: 9.12 Total Reward: 912.0\n",
      "23/100: Average Reward: 9.59 Total Reward: 959.0\n",
      "24/100: Average Reward: 96.14 Total Reward: 9614.0\n",
      "25/100: Average Reward: 36.45 Total Reward: 3645.0\n",
      "26/100: Average Reward: 131.68 Total Reward: 13168.0\n",
      "27/100: Average Reward: 362.77 Total Reward: 36277.0\n",
      "28/100: Average Reward: 9.11 Total Reward: 911.0\n",
      "29/100: Average Reward: 9.64 Total Reward: 964.0\n",
      "30/100: Average Reward: 27.31 Total Reward: 2731.0\n",
      "31/100: Average Reward: 9.62 Total Reward: 962.0\n",
      "32/100: Average Reward: 9.47 Total Reward: 947.0\n",
      "33/100: Average Reward: 118.87 Total Reward: 11887.0\n",
      "34/100: Average Reward: 8.91 Total Reward: 891.0\n",
      "35/100: Average Reward: 8.98 Total Reward: 898.0\n",
      "36/100: Average Reward: 9.83 Total Reward: 983.0\n",
      "37/100: Average Reward: 51.92 Total Reward: 5192.0\n",
      "38/100: Average Reward: 158.13 Total Reward: 15813.0\n",
      "39/100: Average Reward: 48.1 Total Reward: 4810.0\n",
      "40/100: Average Reward: 9.07 Total Reward: 907.0\n",
      "41/100: Average Reward: 8.98 Total Reward: 898.0\n",
      "42/100: Average Reward: 176.68 Total Reward: 17668.0\n",
      "43/100: Average Reward: 9.47 Total Reward: 947.0\n",
      "44/100: Average Reward: 39.15 Total Reward: 3915.0\n",
      "45/100: Average Reward: 380.96 Total Reward: 38096.0\n",
      "46/100: Average Reward: 643.59 Total Reward: 64359.0\n",
      "47/100: Average Reward: 332.07 Total Reward: 33207.0\n",
      "48/100: Average Reward: 9.21 Total Reward: 921.0\n",
      "49/100: Average Reward: 8.84 Total Reward: 884.0\n",
      "50/100: Average Reward: 63.82 Total Reward: 6382.0\n",
      "51/100: Average Reward: 9.57 Total Reward: 957.0\n",
      "52/100: Average Reward: 8.89 Total Reward: 889.0\n",
      "53/100: Average Reward: 9.07 Total Reward: 907.0\n",
      "54/100: Average Reward: 9.19 Total Reward: 919.0\n",
      "55/100: Average Reward: 33.64 Total Reward: 3364.0\n",
      "56/100: Average Reward: 46.31 Total Reward: 4631.0\n",
      "57/100: Average Reward: 180.21 Total Reward: 18021.0\n",
      "58/100: Average Reward: 9.66 Total Reward: 966.0\n",
      "59/100: Average Reward: 722.49 Total Reward: 72249.0\n",
      "60/100: Average Reward: 9.06 Total Reward: 906.0\n",
      "61/100: Average Reward: 9.23 Total Reward: 923.0\n",
      "62/100: Average Reward: 9.19 Total Reward: 919.0\n",
      "63/100: Average Reward: 47.16 Total Reward: 4716.0\n",
      "64/100: Average Reward: 9.39 Total Reward: 939.0\n",
      "65/100: Average Reward: 9.19 Total Reward: 919.0\n",
      "66/100: Average Reward: 67.95 Total Reward: 6795.0\n",
      "67/100: Average Reward: 9.1 Total Reward: 910.0\n",
      "68/100: Average Reward: 50.73 Total Reward: 5073.0\n",
      "69/100: Average Reward: 40.03 Total Reward: 4003.0\n",
      "70/100: Average Reward: 51.45 Total Reward: 5145.0\n",
      "71/100: Average Reward: 743.89 Total Reward: 74389.0\n",
      "72/100: Average Reward: 9.53 Total Reward: 953.0\n",
      "73/100: Average Reward: 61.57 Total Reward: 6157.0\n",
      "74/100: Average Reward: 34.81 Total Reward: 3481.0\n",
      "75/100: Average Reward: 9.61 Total Reward: 961.0\n",
      "76/100: Average Reward: 9.4 Total Reward: 940.0\n",
      "77/100: Average Reward: 116.8 Total Reward: 11680.0\n",
      "78/100: Average Reward: 30.02 Total Reward: 3002.0\n",
      "79/100: Average Reward: 9.38 Total Reward: 938.0\n",
      "80/100: Average Reward: 8.98 Total Reward: 898.0\n",
      "81/100: Average Reward: 24.4 Total Reward: 2440.0\n",
      "82/100: Average Reward: 45.15 Total Reward: 4515.0\n",
      "83/100: Average Reward: 504.38 Total Reward: 50438.0\n",
      "84/100: Average Reward: 9.8 Total Reward: 980.0\n",
      "85/100: Average Reward: 9.66 Total Reward: 966.0\n",
      "86/100: Average Reward: 87.19 Total Reward: 8719.0\n",
      "87/100: Average Reward: 25.95 Total Reward: 2595.0\n",
      "88/100: Average Reward: 249.17 Total Reward: 24917.0\n",
      "89/100: Average Reward: 9.42 Total Reward: 942.0\n",
      "90/100: Average Reward: 9.44 Total Reward: 944.0\n",
      "91/100: Average Reward: 9.49 Total Reward: 949.0\n",
      "92/100: Average Reward: 47.21 Total Reward: 4721.0\n",
      "93/100: Average Reward: 9.68 Total Reward: 968.0\n",
      "94/100: Average Reward: 9.48 Total Reward: 948.0\n",
      "95/100: Average Reward: 76.36 Total Reward: 7636.0\n",
      "96/100: Average Reward: 9.24 Total Reward: 924.0\n",
      "97/100: Average Reward: 42.67 Total Reward: 4267.0\n",
      "98/100: Average Reward: 9.2 Total Reward: 920.0\n",
      "99/100: Average Reward: 9.74 Total Reward: 974.0\n",
      "Best Reward: 743.89\n",
      "Best Weight: 71\n"
     ]
    }
   ],
   "source": [
    "for idx,W in enumerate(Ws):\n",
    "    total_reward,average_reward = test_model(W)\n",
    "    print(\"{0}/{1}: Average Reward: {2} Total Reward: {3}\".format(idx, len(Ws), average_reward, total_reward))\n",
    "    if average_reward > best_weight_reward:\n",
    "        best_weight_reward = average_reward\n",
    "        best_weights = W\n",
    "        best_weights_idx = idx\n",
    "print(\"Best Reward:\", best_weight_reward)\n",
    "print(\"Best Weight:\", best_weights_idx)\n",
    "\n",
    "if best_weight_reward > personal_best_reward:\n",
    "    print(\"It's a NEW LAP RECORD!: {0}\".format(best_weight_reward))\n",
    "    print(best_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11573203, -0.28547568,  0.769634  ,  0.14903186])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ws[best_weights_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def render_model(W):\n",
    "    total_reward = 0\n",
    "    num_batches = 5\n",
    "    for i_episode in range(num_batches):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        batch_reward = 0\n",
    "        print(\"{0}/{1}:\".format(i_episode, num_batches))\n",
    "        for _ in range(1000):\n",
    "            #env.render()  # I don't think you can get this to render from MyBinder. :(\n",
    "            action = model_step(W,observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            batch_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(\"{0}/{1}: Batch Reward: {2}\".format(i_episode, num_batches, batch_reward))\n",
    "        total_reward += batch_reward\n",
    "    average_reward = total_reward/num_batches\n",
    "    return total_reward,average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5:\n",
      "0/5: Batch Reward: 1000.0\n",
      "1/5:\n",
      "1/5: Batch Reward: 1000.0\n",
      "2/5:\n",
      "2/5: Batch Reward: 1000.0\n",
      "3/5:\n",
      "3/5: Batch Reward: 1000.0\n",
      "4/5:\n",
      "4/5: Batch Reward: 1000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000.0, 1000.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(Ws[best_weights_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5:\n",
      "0/5: Batch Reward: 1000.0\n",
      "1/5:\n",
      "1/5: Batch Reward: 1000.0\n",
      "2/5:\n",
      "2/5: Batch Reward: 1000.0\n",
      "3/5:\n",
      "3/5: Batch Reward: 1000.0\n",
      "4/5:\n",
      "4/5: Batch Reward: 1000.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000.0, 1000.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "render_model(personal_best_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
